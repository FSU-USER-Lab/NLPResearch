{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import csv\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import utils\n",
    "\n",
    "from tensorflow.keras import Model, Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense, Dropout, Embedding, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = [] # list of strings\n",
    "filename = 'data/smos/smos_data_porter_balanced.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(filename, newline='') as datafile:\n",
    "    data_reader = csv.reader(datafile, delimiter='\\n')\n",
    "    \n",
    "    for row in data_reader:\n",
    "        training_data.append(row[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6146\n"
     ]
    }
   ],
   "source": [
    "print(len(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = []\n",
    "max = 0\n",
    "\n",
    "for seq in training_data:\n",
    "    lengths.append(len(seq.split()))\n",
    "    if max < len(seq.split()):\n",
    "        max = len(seq.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "print(max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [] # list of strings\n",
    "filename = 'data/smos/smos_labels_balanced.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(filename, newline='') as datafile:\n",
    "    data_reader = csv.reader(datafile, delimiter='\\n')\n",
    "    \n",
    "    for row in data_reader:\n",
    "        labels.append(int(row[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6146 files belonging to 2 classes.\n",
      "Found 6146 files belonging to 2 classes.\n",
      "Using 4917 files for training.\n",
      "Found 6146 files belonging to 2 classes.\n",
      "Using 1229 files for validation.\n"
     ]
    }
   ],
   "source": [
    "# Load and split dataset\n",
    "batch_size = 32\n",
    "seed = 123\n",
    "\n",
    "full_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    'data/smos/train_balanced',\n",
    "    batch_size=batch_size,\n",
    "    label_mode='binary',\n",
    "    seed=seed\n",
    ")\n",
    "\n",
    "train_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    'data/smos/train_balanced',\n",
    "    batch_size=batch_size,\n",
    "    label_mode='binary',\n",
    "    validation_split=0.2, \n",
    "    subset='training',\n",
    "    seed=seed\n",
    ")\n",
    "\n",
    "val_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    'data/smos/train_balanced',\n",
    "    batch_size=batch_size,\n",
    "    label_mode='binary',\n",
    "    validation_split=0.2, \n",
    "    subset='validation',\n",
    "    seed=seed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text_batch, label_batch in train_ds.take(1):\n",
    "    for i in range(2):\n",
    "        print(label_batch[i].numpy(), text_batch.numpy()[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the dataset for performance\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size:  1059\n",
      "Avg seq length:  162.92857142857142\n"
     ]
    }
   ],
   "source": [
    "print('Vocab size: ', utils.vocabulary_size(training_data))\n",
    "print('Avg seq length: ', sum(lengths) / len(lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary size and number of words in a sequence.\n",
    "# Using ~avg sequence length of all sequences\n",
    "sequence_length = 200\n",
    "vocab_size = utils.vocabulary_size(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the text vectorization layer to normalize, split, and map strings to \n",
    "# integers. Note that the layer uses the custom standardization defined above. \n",
    "# Set maximum_sequence length as all samples are not of the same length.\n",
    "vectorize_layer = TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length)\n",
    "\n",
    "# Make a text-only dataset (no labels) and call adapt to build the vocabulary.\n",
    "text_ds = full_ds.map(lambda x, y: x)\n",
    "vectorize_layer.adapt(text_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 8\n",
    "\n",
    "# Embed vocabulary into embedding_dim dimensions.\n",
    "# Embedding tutorial uses size, Text Classification tutorial uses size + 1\n",
    "embedding_layer = tf.keras.layers.Embedding(vocab_size + 1, embedding_dim, name='embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    vectorize_layer,\n",
    "    embedding_layer,\n",
    "    #Dropout(0.2),\n",
    "    GlobalAveragePooling1D(),\n",
    "    #Dropout(0.2),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=tf.metrics.BinaryAccuracy(threshold=0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir='logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "  1/154 [..............................] - ETA: 0s - loss: 0.6929 - binary_accuracy: 0.5938WARNING:tensorflow:From /Users/Marlan/.local/share/virtualenvs/Project_2-jWdg5Lnq/lib/python3.8/site-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "  2/154 [..............................] - ETA: 13s - loss: 0.6929 - binary_accuracy: 0.5469WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0104s vs `on_train_batch_end` time: 0.1671s). Check your callbacks.\n",
      "154/154 [==============================] - 7s 44ms/step - loss: 0.6785 - binary_accuracy: 0.5776 - val_loss: 0.6681 - val_binary_accuracy: 0.6005\n",
      "Epoch 2/15\n",
      "154/154 [==============================] - 1s 5ms/step - loss: 0.6413 - binary_accuracy: 0.6264 - val_loss: 0.6232 - val_binary_accuracy: 0.6762\n",
      "Epoch 3/15\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.5770 - binary_accuracy: 0.7370 - val_loss: 0.5598 - val_binary_accuracy: 0.7543\n",
      "Epoch 4/15\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.5130 - binary_accuracy: 0.7698 - val_loss: 0.5131 - val_binary_accuracy: 0.7486\n",
      "Epoch 5/15\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.4739 - binary_accuracy: 0.7724 - val_loss: 0.4884 - val_binary_accuracy: 0.7543\n",
      "Epoch 6/15\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.4531 - binary_accuracy: 0.7775 - val_loss: 0.4751 - val_binary_accuracy: 0.7583\n",
      "Epoch 7/15\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.4410 - binary_accuracy: 0.7793 - val_loss: 0.4673 - val_binary_accuracy: 0.7592\n",
      "Epoch 8/15\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.4331 - binary_accuracy: 0.7799 - val_loss: 0.4620 - val_binary_accuracy: 0.7600\n",
      "Epoch 9/15\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.4274 - binary_accuracy: 0.7824 - val_loss: 0.4583 - val_binary_accuracy: 0.7657\n",
      "Epoch 10/15\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.4231 - binary_accuracy: 0.7824 - val_loss: 0.4555 - val_binary_accuracy: 0.7648\n",
      "Epoch 11/15\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.4198 - binary_accuracy: 0.7836 - val_loss: 0.4533 - val_binary_accuracy: 0.7657\n",
      "Epoch 12/15\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.4171 - binary_accuracy: 0.7828 - val_loss: 0.4515 - val_binary_accuracy: 0.7648\n",
      "Epoch 13/15\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.4149 - binary_accuracy: 0.7830 - val_loss: 0.4501 - val_binary_accuracy: 0.7665\n",
      "Epoch 14/15\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.4131 - binary_accuracy: 0.7828 - val_loss: 0.4488 - val_binary_accuracy: 0.7665\n",
      "Epoch 15/15\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.4114 - binary_accuracy: 0.7834 - val_loss: 0.4477 - val_binary_accuracy: 0.7665\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1360bae20>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds, \n",
    "    epochs=15,\n",
    "    callbacks=[tensorboard_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "text_vectorization (TextVect (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 200, 8)            8480      \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                144       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 8,641\n",
      "Trainable params: 8,641\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4556 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "original_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    'data/smos/train',\n",
    "    batch_size=batch_size,\n",
    "    label_mode='binary',\n",
    "    seed=seed\n",
    ")\n",
    "\n",
    "original_ds = original_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143/143 [==============================] - 11s 76ms/step - loss: 0.5018 - binary_accuracy: 0.7423\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5017554759979248, 0.742317795753479]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(original_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the trained word embeddings\n",
    "weights = model.get_layer('embedding').get_weights()[0]\n",
    "vocab = vectorize_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save embeddings to disk\n",
    "out_vec = io.open('data/smos_porter_vectors.tsv', 'w', encoding='utf-8')\n",
    "out_meta = io.open('data/smos_porter_metadata.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for index, word in enumerate(vocab):\n",
    "    if  index == 0: continue # skip 0, it's padding.\n",
    "    vec = weights[index] \n",
    "    out_vec.write('\\t'.join([str(x) for x in vec]) + '\\n')\n",
    "    out_meta.write(word + '\\n')\n",
    "    \n",
    "out_vec.close()\n",
    "out_meta.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Project_2",
   "language": "python",
   "name": "project_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
