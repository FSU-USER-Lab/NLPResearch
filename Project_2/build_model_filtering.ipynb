{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import io\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import utils\n",
    "import xxhash\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from random import sample, shuffle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, GlobalAveragePooling1D\n",
    "#from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from tensorflow_addons.metrics import F1Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SET = 'albergate'\n",
    "\n",
    "'''\n",
    "Ratio defining how much data is reserved for CV testing. Ex: 0.8 is an 80/20 train/test split\n",
    "Float on interval (0.0, 1.0)\n",
    "'''\n",
    "TRAIN_TEST_SPLIT = 0.9\n",
    "\n",
    "'''\n",
    "Percentage of cross training data to hold out for testing. This test set will not change across folds,\n",
    "but will change across trials.\n",
    "Ex. ratio=0.1 means a 90/10 split where 90% of a cross training dataset is available for training and 10% is held for testing.\n",
    "'''\n",
    "CROSS_TRAIN_HOLDOUT_RATIO = 0.1\n",
    "\n",
    "'''\n",
    "Percentage of dataset to use for cross training. NOTE: this can not be larger that 1 - CROSS_TRAIN_HOLDOUT_RATIO\n",
    "Ex. ratio=0.1 means a training set 10% of the ORIGINAL dataset is used for cross training. The training data will never overlap with the holdout data.\n",
    "\n",
    "'''\n",
    "CROSS_TRAIN_RATIO = 0.1\n",
    "\n",
    "'''\n",
    "Ratio defining how many synthetic minority samples should be created. 1.0 results in a fully balanced set.\n",
    "Float on interval (0.0, 1.0]\n",
    "'''\n",
    "BALANCE_RATIO = 1.0\n",
    "\n",
    "# Number of cross validation folds. Default: 10\n",
    "N_FOLDS = 1 \n",
    "\n",
    "# Number of trials of n fold cv\n",
    "N_TRIALS = 1 \n",
    "\n",
    "# Training epochs\n",
    "N_EPOCHS=10\n",
    "\n",
    "# Training batch size\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Dimension of the embedding layer. \n",
    "EMBEDDING_DIM = 8\n",
    "\n",
    "# Metrics to meature training performance\n",
    "METRICS = ['loss', 'binary_accuracy', 'recall', 'precision', 'f1_score']\n",
    "\n",
    "# Folder to store TF callback logs\n",
    "TENSORBOARD_CALLBACK = tf.keras.callbacks.TensorBoard(log_dir='logs')\n",
    "\n",
    "# Verbosity: 0 = silent, 1 = progress bar, 2 = one line per epoch\n",
    "VERBOSITY = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATHS = {\n",
    "    'albergate': ('data/albergate/albergate_data.txt', 'data/albergate/albergate_labels.txt'),\n",
    "    'eanci': ('data/eanci/eanci_data.txt', 'data/eanci/eanci_labels.txt'),\n",
    "    'etour': ('data/etour/etour_data.txt', 'data/etour/etour_labels.txt'),\n",
    "    'itrust': ('data/itrust/itrust_data.txt', 'data/itrust/itrust_labels.txt'),\n",
    "    'kepler': ('data/kepler/kepler_data.txt', 'data/kepler/kepler_labels.txt'),\n",
    "    'modis': ('data/modis/modis_data.txt', 'data/modis/modis_labels.txt'),\n",
    "    'smos': ('data/smos/smos_data.txt', 'data/smos/smos_labels.txt')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_output(trial_averages):\n",
    "    result_averages = {}\n",
    "\n",
    "    # Stores metric averages across all trials\n",
    "    for metric, results in trial_averages.items():\n",
    "        result_averages[metric+'_avg'] = sum(results)/len(results)\n",
    "    \n",
    "    return pd.DataFrame(trial_averages), pd.DataFrame(result_averages, index=result_averages.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_f1scores(averages, datasets, phase):    \n",
    "    f1_scores = []\n",
    "    \n",
    "    # Create lists for axes\n",
    "    datasets = sorted(datasets)\n",
    "    x_datasets = datasets.copy()\n",
    "    datasets.reverse()\n",
    "    y_datasets = datasets.copy()\n",
    "    \n",
    "    for cross_train_set in y_datasets:\n",
    "        temp = []\n",
    "        for dataset in x_datasets:\n",
    "            \n",
    "            if dataset == cross_train_set:\n",
    "                temp.append(float(format(0, '.2f')))\n",
    "            else:\n",
    "                temp.append(float(format(averages[dataset+'|'+cross_train_set], '.2f')))\n",
    "\n",
    "        f1_scores.append(temp)\n",
    "        \n",
    "    f1_scores = np.array(f1_scores)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    im = ax.imshow(f1_scores, cmap='Greys')\n",
    "    \n",
    "    # Label axes\n",
    "    plt.xlabel('Primary Set')\n",
    "    plt.ylabel('Cross Train Set')\n",
    "    \n",
    "    # Set chart title\n",
    "    ax.set_title(phase.upper()+' '+\"F1 Scores\")\n",
    "\n",
    "    # Show all axis ticks\n",
    "    ax.set_xticks(np.arange(len(x_datasets)))\n",
    "    ax.set_yticks(np.arange(len(y_datasets)))\n",
    "    \n",
    "    # Label axis ticks\n",
    "    ax.set_xticklabels(x_datasets)\n",
    "    ax.set_yticklabels(y_datasets)\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    for i in range(len(y_datasets)):\n",
    "        for j in range(len(x_datasets)):\n",
    "            text = ax.text(j, i, f1_scores[i, j],\n",
    "                           ha=\"center\", va=\"center\", color=\"r\")\n",
    "\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    plt.savefig('trials/'+phase+'_heatmap')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(fold_results, history):\n",
    "    # Append current fold results to trial_history dict\n",
    "    # Metric names are appended with a _(run number) each trial, hence the nested for loop\n",
    "    for current_metric, results in fold_results.items():\n",
    "        for metric in history.keys():\n",
    "            if metric in current_metric:\n",
    "                # F1Score stores results as a list of lists instead of list of floats\n",
    "                if metric == 'f1_score':\n",
    "                    try:\n",
    "                        total=0\n",
    "                        for result in results:\n",
    "                            total+=result[0]\n",
    "                        \n",
    "                        history[metric].append(total/len(results))\n",
    "                    except:\n",
    "                        history[metric].append(results[0])\n",
    "                    \n",
    "                else:\n",
    "                    try:\n",
    "                        history[metric].append(sum(results)/len(results))\n",
    "                    except:\n",
    "                        history[metric].append(results)\n",
    "\n",
    "                break\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Load all datasets and associated labels.\n",
    "'''\n",
    "\n",
    "DATASETS = {}\n",
    "\n",
    "for set_name, paths in DATASET_PATHS.items():\n",
    "    data = []\n",
    "    labels = []\n",
    "    \n",
    "    # Load dataset metadocs\n",
    "    with open(paths[0], newline='') as datafile:\n",
    "        data_reader = csv.reader(datafile, delimiter='\\n')\n",
    "\n",
    "        for row in data_reader:\n",
    "            data.append(row[0])\n",
    "            \n",
    "    # Load dataset labels\n",
    "    with open(paths[1], newline='') as labelfile:\n",
    "        label_reader = csv.reader(labelfile, delimiter='\\n')\n",
    "\n",
    "        for row in label_reader:\n",
    "            labels.append(int(row[0]))\n",
    "            \n",
    "    DATASETS[set_name] = (data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "All datasets will be concatenated into a single corpus.\n",
    "We want to track where each set begins and ends in the corpus\n",
    "in order to build the test and train sets.\n",
    "'''\n",
    "\n",
    "index = -1\n",
    "DATASET_INDICES = {}\n",
    "\n",
    "for set_name, data in DATASETS.items():\n",
    "    DATASET_INDICES[set_name] = (index+1, index+len(data[0]))\n",
    "    print(set_name, DATASET_INDICES[set_name])\n",
    "    index += len(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Create the corpus.\n",
    "'''\n",
    "\n",
    "num_docs = 0\n",
    "corpus = []\n",
    "vocabulary = []\n",
    "\n",
    "for dataset in DATASETS.keys():\n",
    "    #print(dataset)\n",
    "    corpus.extend(DATASETS[dataset][0])\n",
    "    vocabulary.extend(utils.get_vocabulary(DATASETS[dataset][0]))\n",
    "    num_docs += len(DATASETS[dataset][0])\n",
    "    \n",
    "vocabulary = list(set(vocabulary))\n",
    "print('Docs processed:', num_docs)\n",
    "print('Corpus size:', len(corpus))\n",
    "print('Vocabulary size:', len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset, indices in DATASET_INDICES.items():\n",
    "    print(dataset, corpus[indices[0]], corpus[indices[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Create tf-idf vectorizer and encode corpus.\n",
    "'''\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "# Generate numpy matrix of encoded corpus\n",
    "ENCODED_DATASETS = vectorizer.fit_transform(corpus)\n",
    "# Convert from numpy matrix to numpy array so we can iterate over the rows\n",
    "ENCODED_DATASETS = ENCODED_DATASETS.A \n",
    "\n",
    "print('Encoded corpus size:', len(ENCODED_DATASETS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Sanity check that no encoded metadoc is all 0s.\n",
    "'''\n",
    "\n",
    "total = 0\n",
    "\n",
    "for metadoc in ENCODED_DATASETS:\n",
    "    if np.count_nonzero(metadoc) == 0:\n",
    "        total += 1\n",
    "        \n",
    "print('Meta docs with all 0s:', total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Check for words lost in tf-idf encoding.\n",
    "Reference sklearn documentation for tf-idf specs.\n",
    "'''\n",
    "\n",
    "encoded_vocabulary = vectorizer.get_feature_names()\n",
    "\n",
    "print('Encoded vocabulary size:', len(encoded_vocabulary))\n",
    "'''print('Terms missing from encoded vocabulary:')\n",
    "\n",
    "for term in vocabulary:\n",
    "    if term not in encoded_vocabulary:\n",
    "        print(term)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Check for repeated items in encoded set.\n",
    "Store duplicates to avoid using duplicates in the test and train sets.\n",
    "'''\n",
    "\n",
    "unq, count = np.unique(ENCODED_DATASETS, axis=0, return_counts=True)\n",
    "repeated_groups = unq[count > 1]\n",
    "repeated_indices = []\n",
    "\n",
    "for repeated_group in repeated_groups:\n",
    "    repeated_idx = np.argwhere(np.all(ENCODED_DATASETS == repeated_group, axis=1))\n",
    "    #print(repeated_idx.ravel())\n",
    "    for index in repeated_idx.ravel():\n",
    "        repeated_indices.append(index)\n",
    "\n",
    "print('Number of repeated items:', len(repeated_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Collision info\n",
    "'''\n",
    "\n",
    "ENCODED_COLLISIONS = {}\n",
    "row_num,k = 0,0\n",
    "set_names = list(DATASETS.keys())\n",
    "\n",
    "for i in range(len(ENCODED_DATASETS)):\n",
    "    \n",
    "    # If we are done with current set, move to the next\n",
    "    if row_num >= len(DATASETS[set_names[k]][0]):\n",
    "        row_num = 0\n",
    "        k += 1\n",
    "\n",
    "    if str(hash(bytes(ENCODED_DATASETS[i].data))) in ENCODED_COLLISIONS.keys():\n",
    "        ENCODED_COLLISIONS[str(hash(bytes(ENCODED_DATASETS[i].data)))].append((set_names[k], row_num))\n",
    "    else:\n",
    "        ENCODED_COLLISIONS[str(hash(bytes(ENCODED_DATASETS[i].data)))] = [(set_names[k], row_num)]\n",
    "                                                                          \n",
    "    row_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totals = dict.fromkeys(set_names, 0)\n",
    "\n",
    "for coll_list in ENCODED_COLLISIONS.values():\n",
    "    if len(coll_list) > 1:\n",
    "        for entry in coll_list:\n",
    "            totals[entry[0]] += 1\n",
    "        #print(coll_list)\n",
    "        \n",
    "print(totals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "We need to retrieve the label of an encoded metadoc after clustering.\n",
    "Storing labels in dict using hashed encoded metadocs as keys.\n",
    "'''\n",
    "\n",
    "ENCODED_LABELS = {}\n",
    "row_num,k = 0,0\n",
    "set_names = list(DATASETS.keys())\n",
    "\n",
    "for i in range(len(ENCODED_DATASETS)):\n",
    "    # If the encoded metadoc is a repeated item, skip it\n",
    "    if i in repeated_indices:\n",
    "        row_num += 1\n",
    "        continue\n",
    "    \n",
    "    # If we are done with current set, move to the next\n",
    "    if row_num >= len(DATASETS[set_names[k]][0]):\n",
    "        row_num = 0\n",
    "        k += 1\n",
    "        \n",
    "    if i == DATASET_INDICES[set_names[k]][0] or i == DATASET_INDICES[set_names[k]][1]:\n",
    "        print(set_names[k], DATASETS[set_names[k]][1][row_num])\n",
    "    \n",
    "    # If we have a hash collision, stop iterating\n",
    "    if xxhash.xxh32(bytes(ENCODED_DATASETS[i].data)).digest() in ENCODED_LABELS.keys():\n",
    "        print('Hash collision encountered on key:', xxhash.xxh32(bytes(ENCODED_DATASETS[i].data)).digest())\n",
    "        #break\n",
    "         \n",
    "    \n",
    "        \n",
    "    # Store label of encoded metadoc\n",
    "    ENCODED_DATASETS[i].flags.writeable = False\n",
    "    ENCODED_LABELS[xxhash.xxh32(bytes(ENCODED_DATASETS[i].data)).digest()] = DATASETS[set_names[k]][1][row_num]\n",
    "    \n",
    "    row_num += 1\n",
    "    \n",
    "print('Encoded labels stored:', len(ENCODED_LABELS.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Build train and test set indices.\n",
    "'''\n",
    "\n",
    "train_indices = []\n",
    "test_indices = []\n",
    "\n",
    "for i in range(DATASET_INDICES[TEST_SET][0], DATASET_INDICES[TEST_SET][1]+1):\n",
    "    # If the encoded metadoc is a repeated item, skip it\n",
    "    if i in repeated_indices:\n",
    "        continue\n",
    "    test_indices.append(i)\n",
    "    \n",
    "for set_name in DATASET_INDICES.keys():\n",
    "    if set_name == TEST_SET:\n",
    "        continue\n",
    "    else:\n",
    "        for i in range(DATASET_INDICES[set_name][0], DATASET_INDICES[set_name][1]+1):\n",
    "            # If the encoded metadoc is a repeated item, skip it\n",
    "            if i in repeated_indices:\n",
    "                continue\n",
    "            train_indices.append(i)\n",
    "        \n",
    "print('Training data available:', len(train_indices))\n",
    "print('Test set size:', len(test_indices))\n",
    "print('Total set size:', len(train_indices) + len(test_indices))\n",
    "\n",
    "# Convert to numpy arrays for easier slicing\n",
    "train_indices = np.array(train_indices)\n",
    "test_indices = np.array(test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "for index in train_indices:\n",
    "    if index in test_indices:\n",
    "        i += 1\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Calculate the centroid of the test set in order to perform kNN clustering.\n",
    "'''\n",
    "\n",
    "test_set_centroid = np.mean(ENCODED_DATASETS[test_indices], axis=0)\n",
    "test_set_centroid = test_set_centroid.reshape(1, -1)\n",
    "\n",
    "print('Centroid dimensions:', test_set_centroid.shape)\n",
    "print('Number of nonzero entries(sanity check):', np.count_nonzero(test_set_centroid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Use kNN to generate training dataset.\n",
    "'''\n",
    "\n",
    "# We need to copy the data since knn returns indices correlated to the training set passed in\n",
    "training_data = np.copy(ENCODED_DATASETS[train_indices])\n",
    "\n",
    "knn_extractor = NearestNeighbors(n_neighbors=int(len(training_data) * 0.3))\n",
    "knn_extractor.fit(training_data)\n",
    "\n",
    "knn_indices = knn_extractor.kneighbors(test_set_centroid, return_distance=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Create train and test set label arrays.\n",
    "'''\n",
    "\n",
    "training_labels = []\n",
    "test_labels = []\n",
    "\n",
    "for index in np.nditer(knn_indices):\n",
    "    count += 1\n",
    "    #print(train_set[index])\n",
    "    #train_set[index].flags.writeable = False\n",
    "    training_labels.append(ENCODED_LABELS[xxhash.xxh32(bytes(training_data[index].data)).digest()])\n",
    "    \n",
    "for index in np.nditer(test_indices):\n",
    "    #print(train_set[index])\n",
    "    #train_set[index].flags.writeable = False\n",
    "    test_labels.append(ENCODED_LABELS[xxhash.xxh32(bytes(ENCODED_DATASETS[index].data)).digest()])\n",
    "    \n",
    "print('Training labels:', len(training_labels))\n",
    "print('Test labels:', len(test_labels))\n",
    "\n",
    "training_labels = np.array(training_labels)\n",
    "test_labels = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = []\n",
    "\n",
    "for index in np.nditer(knn_indices):\n",
    "    temp.append(index)\n",
    "    \n",
    "knn_indices = np.array(temp)\n",
    "print(knn_indices.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Perform SMOTE on loaded dataset\n",
    "balanced_training_data, balanced_training_labels = SMOTE(sampling_strategy=BALANCE_RATIO).fit_resample(training_data[knn_indices], training_labels)\n",
    "print('Balanced training set size:', len(balanced_training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedShuffleSplit(n_splits=N_FOLDS, train_size=TRAIN_TEST_SPLIT, random_state=123)\n",
    "k=1\n",
    "\n",
    "#for train, test in skf.split(training_data[knn_indices], training_labels):\n",
    "for train, test in skf.split(balanced_training_data, balanced_training_labels):\n",
    "\n",
    "    # This will cause the model to build an index of strings to integers.\n",
    "    # Per TF: It's important to only use training data when creating vocabulary (using the test set would leak information).\n",
    "    #vectorize_layer.set_vocabulary(utils.get_vocabulary(training_data[train]))\n",
    "    #input_dim = len(vectorize_layer.get_vocabulary())\n",
    "\n",
    "    # Embed vocabulary into embedding_dim dimensions.\n",
    "    # Embedding tutorial uses size, Text Classification tutorial uses size + 1 for input_dim\n",
    "    embedding_layer = tf.keras.layers.Embedding(len(encoded_vocabulary), EMBEDDING_DIM, name='embedding')\n",
    "\n",
    "    # Define model structure\n",
    "    model = Sequential([\n",
    "        #vectorize_layer,\n",
    "        embedding_layer,\n",
    "        #Dropout(0.2),\n",
    "        GlobalAveragePooling1D(),\n",
    "        #Dropout(0.2),\n",
    "        Dense(16, activation='relu'),\n",
    "        #Dense(128, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    # Create model\n",
    "    model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=False), # tutorials use true for training, false for production\n",
    "              metrics=[\n",
    "                  tf.metrics.BinaryAccuracy(threshold=0.5),\n",
    "                  tf.keras.metrics.Recall(),\n",
    "                  tf.keras.metrics.Precision(),\n",
    "                  F1Score(1, threshold=0.5)\n",
    "              ]\n",
    "    )\n",
    "\n",
    "    print('\\n\\n*************** FOLD %d ***************' %k)\n",
    "\n",
    "    print('\\n******* TRAIN *******')\n",
    "    # Train model\n",
    "    train_results = model.fit(\n",
    "        balanced_training_data[train],\n",
    "        balanced_training_labels[train],\n",
    "        batch_size=BATCH_SIZE, \n",
    "        epochs=N_EPOCHS,\n",
    "        #callbacks=[TENSORBOARD_CALLBACK],\n",
    "        verbose=VERBOSITY\n",
    "    )\n",
    "\n",
    "    print('\\n******* VALIDATION *******')\n",
    "    # Test model with validation data\n",
    "    validation_results = model.evaluate(\n",
    "        balanced_training_data[test],\n",
    "        balanced_training_labels[test],\n",
    "        #callbacks=[TENSORBOARD_CALLBACK],\n",
    "        return_dict=True,\n",
    "        verbose=VERBOSITY\n",
    "    )\n",
    "\n",
    "    print('\\n******* FILTER TEST *******')\n",
    "    # Test model with cross train data\n",
    "    ct_results = model.evaluate(\n",
    "        ENCODED_DATASETS[test_indices],\n",
    "        test_labels,\n",
    "        #callbacks=[TENSORBOARD_CALLBACK],\n",
    "        return_dict=True,\n",
    "        verbose=VERBOSITY\n",
    "    )\n",
    "    k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_CT_SEEDS = set()\n",
    "RANDOM_CV_SEEDS = set()\n",
    "\n",
    "# Generate list of unique random seeds to use with StratifiedShuffleSplit objects\n",
    "while len(RANDOM_CT_SEEDS) < (len(DATASETS.keys()) * (len(DATASETS.keys())-1) * N_TRIALS):\n",
    "    RANDOM_CT_SEEDS.add(np.random.randint(10000))\n",
    "\n",
    "# Generate list of unique random seeds to use with StratifiedShuffleSplit objects\n",
    "while len(RANDOM_CV_SEEDS) < N_TRIALS:\n",
    "    RANDOM_CV_SEEDS.add(np.random.randint(1000))\n",
    "\n",
    "RANDOM_CT_SEEDS = iter(list(RANDOM_CT_SEEDS))\n",
    "RANDOM_CV_SEEDS = list(RANDOM_CV_SEEDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use the text vectorization layer to normalize, split, and map strings to integers. \n",
    "vectorize_layer = TextVectorization()\n",
    "\n",
    "train_f1scores = {}\n",
    "validation_f1scores = {}\n",
    "ct_f1scores = {}\n",
    "\n",
    "for primary_set, data_labels in DATASETS.items():\n",
    "    for cross_train_set, ct_data_labels in DATASETS.items():\n",
    "        if primary_set == cross_train_set:\n",
    "            pass\n",
    "        else:\n",
    "            print('\\n\\n*************************', primary_set.upper(), '+', cross_train_set.upper(), '*************************')\n",
    "            \n",
    "            # Store metric averages for each trial\n",
    "            train_averages = dict([(metric,[]) for metric in METRICS])\n",
    "            validation_averages = dict([(metric,[]) for metric in METRICS])\n",
    "            ct_averages = dict([(metric,[]) for metric in METRICS])\n",
    "\n",
    "            # Peform N_TRIALS of N_FOLDS CV\n",
    "            for i,RANDOM_SEED in enumerate(RANDOM_CV_SEEDS):\n",
    "                print('\\n\\n******************** TRIAL %d ********************' %(i+1))\n",
    "                \n",
    "                training_data, labels, ct_test_data, ct_test_labels = mix_data(data_labels, ct_data_labels, CROSS_TRAIN_HOLDOUT_RATIO, next(RANDOM_CT_SEEDS))\n",
    "\n",
    "                # Convert data and labels to numpy arrays for training and testing\n",
    "                #training_data = np.array(training_data, dtype=object)\n",
    "                #labels = np.array(labels)\n",
    "                \n",
    "                k=1 # Fold counter\n",
    "                # Store metric averages for each fold of a single trial\n",
    "                train_history = dict([(metric,[]) for metric in METRICS])\n",
    "                validation_history = dict([(metric,[]) for metric in METRICS])\n",
    "                ct_history = dict([(metric,[]) for metric in METRICS])\n",
    "                \n",
    "                skf = StratifiedShuffleSplit(n_splits=N_FOLDS, train_size=TRAIN_TEST_SPLIT, random_state=RANDOM_SEED)\n",
    "\n",
    "                for train, test in skf.split(training_data, labels):\n",
    "\n",
    "                    # This will cause the model to build an index of strings to integers.\n",
    "                    # Per TF: It's important to only use training data when creating vocabulary (using the test set would leak information).\n",
    "                    vectorize_layer.set_vocabulary(utils.get_vocabulary(training_data[train]))\n",
    "                    input_dim = len(vectorize_layer.get_vocabulary())\n",
    "\n",
    "                    # Embed vocabulary into embedding_dim dimensions.\n",
    "                    # Embedding tutorial uses size, Text Classification tutorial uses size + 1 for input_dim\n",
    "                    embedding_layer = tf.keras.layers.Embedding(input_dim, EMBEDDING_DIM, name='embedding')\n",
    "\n",
    "                    # Define model structure\n",
    "                    model = Sequential([\n",
    "                        vectorize_layer,\n",
    "                        embedding_layer,\n",
    "                        #Dropout(0.2),\n",
    "                        GlobalAveragePooling1D(),\n",
    "                        #Dropout(0.2),\n",
    "                        Dense(16, activation='relu'),\n",
    "                        Dense(1, activation='sigmoid')\n",
    "                    ])\n",
    "\n",
    "                    # Create model\n",
    "                    model.compile(optimizer='adam',\n",
    "                              loss=tf.keras.losses.BinaryCrossentropy(from_logits=False), # tutorials use true for training, false for production\n",
    "                              metrics=[\n",
    "                                  tf.metrics.BinaryAccuracy(threshold=0.5),\n",
    "                                  tf.keras.metrics.Recall(),\n",
    "                                  tf.keras.metrics.Precision(),\n",
    "                                  F1Score(1, threshold=0.5)\n",
    "                              ]\n",
    "                    )\n",
    "\n",
    "                    print('\\n\\n*************** FOLD %d ***************' %k)\n",
    "\n",
    "                    print('\\n******* TRAIN *******')\n",
    "                    # Train model\n",
    "                    train_results = model.fit(\n",
    "                        training_data[train],\n",
    "                        labels[train],\n",
    "                        batch_size=BATCH_SIZE, \n",
    "                        epochs=N_EPOCHS,\n",
    "                        callbacks=[TENSORBOARD_CALLBACK],\n",
    "                        verbose=VERBOSITY\n",
    "                    )\n",
    "\n",
    "                    print('\\n******* VALIDATION *******')\n",
    "                    # Test model with validation data\n",
    "                    validation_results = model.evaluate(\n",
    "                        training_data[test],\n",
    "                        labels[test],\n",
    "                        callbacks=[TENSORBOARD_CALLBACK],\n",
    "                        return_dict=True,\n",
    "                        verbose=VERBOSITY\n",
    "                    )\n",
    "\n",
    "                    print('\\n******* CT TEST *******')\n",
    "                    # Test model with cross train data\n",
    "                    ct_results = model.evaluate(\n",
    "                        ct_test_data,\n",
    "                        ct_test_labels,\n",
    "                        callbacks=[TENSORBOARD_CALLBACK],\n",
    "                        return_dict=True,\n",
    "                        verbose=VERBOSITY\n",
    "                    )\n",
    "                    \n",
    "                    train_history = save_results(train_results.history, train_history)\n",
    "                    validation_history = save_results(validation_results, validation_history)\n",
    "                    ct_history = save_results(ct_results, ct_history)\n",
    "\n",
    "                    # If we are in the last fold of the trial, average the metric results \n",
    "                    # across all n folds and append to trial_averages\n",
    "                    if k == N_FOLDS:\n",
    "                        for metric, results in train_history.items():\n",
    "                            train_averages[metric].append(sum(results)/len(results))\n",
    "                        for metric, results in validation_history.items():\n",
    "                            validation_averages[metric].append(sum(results)/len(results))\n",
    "                        for metric, results in ct_history.items():\n",
    "                            ct_averages[metric].append(sum(results)/len(results))\n",
    "\n",
    "                    k += 1\n",
    "\n",
    "            RESULTS_FILE = 'trials/'+primary_set+'_'+str(CROSS_TRAIN_RATIO)+'_'+cross_train_set+'_'+str(N_TRIALS)+'_T_'+ str(N_FOLDS)+'_fCV.xlsx'\n",
    "            \n",
    "            '''\n",
    "            Write all results to an excel file.\n",
    "            The first sheet shows metric averages for each trial. The second sheet contains the averages across all trials.\n",
    "            '''\n",
    "            trial_table, averages_table = format_output(train_averages)\n",
    "            \n",
    "            with pd.ExcelWriter(RESULTS_FILE) as writer:\n",
    "                trial_table.to_excel(writer, sheet_name='Training Trials')\n",
    "\n",
    "            with pd.ExcelWriter(RESULTS_FILE, mode='a') as writer:\n",
    "                averages_table.iloc[0].to_excel(writer, sheet_name='Training Averages', header=False)\n",
    "                \n",
    "            trial_table, averages_table = format_output(validation_averages)\n",
    "            \n",
    "            with pd.ExcelWriter(RESULTS_FILE, mode='a') as writer:\n",
    "                trial_table.to_excel(writer, sheet_name='Validation Trials')\n",
    "\n",
    "            with pd.ExcelWriter(RESULTS_FILE, mode='a') as writer:\n",
    "                averages_table.iloc[0].to_excel(writer, sheet_name='Validation Averages', header=False)\n",
    "                \n",
    "            trial_table, averages_table = format_output(ct_averages)\n",
    "            \n",
    "            with pd.ExcelWriter(RESULTS_FILE, mode='a') as writer:\n",
    "                trial_table.to_excel(writer, sheet_name='CT Trials')\n",
    "\n",
    "            with pd.ExcelWriter(RESULTS_FILE, mode='a') as writer:\n",
    "                averages_table.iloc[0].to_excel(writer, sheet_name='CT Averages', header=False)\n",
    "                \n",
    "            train_f1scores[primary_set+'|'+cross_train_set] = sum(train_averages['f1_score'])/len(train_averages['f1_score'])\n",
    "            validation_f1scores[primary_set+'|'+cross_train_set] = sum(validation_averages['f1_score'])/len(validation_averages['f1_score'])\n",
    "            ct_f1scores[primary_set+'|'+cross_train_set] = sum(ct_averages['f1_score'])/len(ct_averages['f1_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_f1scores(train_f1scores, list(DATASETS.keys()), 'training')\n",
    "plot_f1scores(validation_f1scores, list(DATASETS.keys()), 'validation')\n",
    "plot_f1scores(ct_f1scores, list(DATASETS.keys()), 'cross training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display model information\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the trained word embeddings\n",
    "weights = model.get_layer('embedding').get_weights()[0]\n",
    "vocab = vectorize_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_VECTOR_FILE = 'data/smos/smos_porter_balanced_vectors.tsv'\n",
    "OUTPUT_METADATA_FILE = 'data/smos/smos_porter_balanced_metadata.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save embeddings to disk\n",
    "out_vec = io.open(OUTPUT_VECTOR_FILE, 'w', encoding='utf-8')\n",
    "out_meta = io.open(OUTPUT_METADATA_FILE, 'w', encoding='utf-8')\n",
    "\n",
    "for index, word in enumerate(vocab):\n",
    "    if  index == 0: continue # skip 0, it's padding.\n",
    "    vec = weights[index] \n",
    "    out_vec.write('\\t'.join([str(x) for x in vec]) + '\\n')\n",
    "    out_meta.write(word + '\\n')\n",
    "    \n",
    "out_vec.close()\n",
    "out_meta.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' OLD VERSION '''\n",
    "ENCODED_LABELS = {}\n",
    "num_docs = 0\n",
    "collision = False\n",
    "\n",
    "for name, metadocs in ENCODED_DATASETS.items():\n",
    "    # hash each metadoc we are using, and store the label associated with the hash\n",
    "    # will use to retrieve labels after clustering\n",
    "    print(name, metadocs.shape)\n",
    "    for i,metadoc in enumerate(metadocs):\n",
    "        num_docs += 1\n",
    "        metadoc.flags.writeable = False\n",
    "        if str(hash(bytes(metadoc.data))) in ENCODED_LABELS.keys():\n",
    "            print('hash collision')\n",
    "            collision = True\n",
    "            break\n",
    "        ENCODED_LABELS[str(hash(bytes(metadoc.data)))] = DATASETS[name][1][i]\n",
    "        #print(DATASETS[name][1][i])\n",
    "        #break\n",
    "    if collision:\n",
    "        print('exiting outer loop')\n",
    "        break\n",
    "print(num_docs, len(ENCODED_LABELS.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Project_2",
   "language": "python",
   "name": "project_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
