{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import utils\n",
    "\n",
    "from random import sample, shuffle\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from tensorflow.keras import Model, Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense, Dropout, Embedding, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from tensorflow_addons.metrics import F1Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mix_data(data_labels, ct_data_labels, CROSS_TRAIN_HOLDOUT_RATIO, CT_SEED):\n",
    "    skf = StratifiedShuffleSplit(n_splits=1, train_size=CROSS_TRAIN_HOLDOUT_RATIO, random_state=CT_SEED)\n",
    "    ct_test_data, ct_test_labels, ct_train_data, ct_train_labels = None, None, None, None\n",
    "\n",
    "    ct_all_data = np.array(ct_data_labels[0], dtype=object)\n",
    "    ct_all_labels = np.array(ct_data_labels[1])\n",
    "\n",
    "    for train, test in skf.split(ct_all_data, ct_all_labels):\n",
    "        ct_test_data = ct_all_data[test]\n",
    "        ct_test_labels = ct_all_labels[test]\n",
    "\n",
    "        ct_train_data = [ct_data_labels[0][index] for index in train]\n",
    "        ct_train_labels = [ct_data_labels[1][index] for index in train]\n",
    "\n",
    "    training_data = data_labels[0].copy()\n",
    "    labels = data_labels[1].copy()\n",
    "\n",
    "\n",
    "\n",
    "    print(primary_set, ':', len(training_data))\n",
    "    print(cross_train_set, ':', len(ct_all_data))\n",
    "\n",
    "    for datapoint, label in sample(list(zip(ct_train_data, ct_train_labels)), k=int(len(ct_all_data)*CROSS_TRAIN_RATIO)):\n",
    "        training_data.append(datapoint)\n",
    "        labels.append(label)\n",
    "\n",
    "    shuffled = list(zip(training_data, labels))\n",
    "    shuffle(shuffled)\n",
    "    \n",
    "    training_data.clear()\n",
    "    labels.clear()\n",
    "    \n",
    "    for datapoint, label in shuffled:\n",
    "        training_data.append(datapoint)\n",
    "        labels.append(label)\n",
    "    \n",
    "    print('Mixed :', len(training_data))\n",
    "    \n",
    "    return training_data, labels, ct_test_data, ct_test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(fold_results, history):\n",
    "    # Append current fold results to trial_history dict\n",
    "    # Metric names are appended with a _(run number) each trial, hence the nested for loop\n",
    "    for current_metric, results in fold_results.items():\n",
    "        for metric in history.keys():\n",
    "            if metric in current_metric:\n",
    "                # F1Score stores results as a list of lists instead of list of floats\n",
    "                if metric == 'f1_score':\n",
    "                    try:\n",
    "                        total=0\n",
    "                        for result in results:\n",
    "                            total+=result[0]\n",
    "                        \n",
    "                        history[metric].append(total/len(results))\n",
    "                    except:\n",
    "                        history[metric].append(results[0])\n",
    "                    \n",
    "                else:\n",
    "                    try:\n",
    "                        history[metric].append(sum(results)/len(results))\n",
    "                    except:\n",
    "                        history[metric].append(results)\n",
    "\n",
    "                break\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_output(trial_averages):\n",
    "    result_averages = {}\n",
    "\n",
    "    # Stores metric averages across all trials\n",
    "    for metric, results in trial_averages.items():\n",
    "        result_averages[metric+'_avg'] = sum(results)/len(results)\n",
    "    \n",
    "    return pd.DataFrame(trial_averages), pd.DataFrame(result_averages, index=result_averages.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATHS = {\n",
    "    'albergate': ('data/albergate/albergate_data_balanced.txt', 'data/albergate/albergate_labels_balanced.txt'),\n",
    "    'eanci': ('data/eanci/eanci_data_balanced.txt', 'data/eanci/eanci_labels_balanced.txt'),\n",
    "    'etour': ('data/etour/etour_data_balanced.txt', 'data/etour/etour_data_balanced.txt'),\n",
    "    'itrust': ('data/itrust/itrust_data_balanced.txt', 'data/itrust/itrust_labels_balanced.txt'),\n",
    "    'kepler': ('data/kepler/kepler_data_balanced.txt', 'data/kepler/kepler_labels_balanced.txt'),\n",
    "    'modis': ('data/modis/modis_data_balanced.txt', 'data/modis/modis_labels_balanced.txt'),\n",
    "    'smos': ('data/smos/smos_data_balanced.txt', 'data/smos/smos_labels_balanced.txt')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_VECTOR_FILE = 'data/smos/smos_porter_balanced_vectors.tsv'\n",
    "OUTPUT_METADATA_FILE = 'data/smos/smos_porter_balanced_metadata.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Ratio defining how much data is reserved for CV testing. Ex: 0.8 is an 80/20 train/test split\n",
    "Float on interval (0.0, 1.0)\n",
    "'''\n",
    "TRAIN_TEST_SPLIT = 0.8\n",
    "\n",
    "'''\n",
    "Percentage of cross training data to hold out for testing. This test set will not change across folds,\n",
    "but will change across trials.\n",
    "Ex. ratio=0.1 means a 90/10 split where 90% of a cross training dataset is available for training and 10% is held for testing.\n",
    "'''\n",
    "CROSS_TRAIN_HOLDOUT_RATIO = 0.1\n",
    "\n",
    "'''\n",
    "Percentage of dataset to use for cross training. NOTE: this can not be larger that 1 - CROSS_TRAIN_HOLDOUT_RATIO\n",
    "Ex. ratio=0.1 means a training set 10% of the ORIGINAL dataset is used for cross training. The training data will never overlap with the holdout data.\n",
    "\n",
    "'''\n",
    "CROSS_TRAIN_RATIO = 0.1\n",
    "\n",
    "# Number of cross validation folds. Default: 10\n",
    "N_FOLDS = 10 \n",
    "\n",
    "# Number of trials of n fold cv\n",
    "N_TRIALS = 10 \n",
    "\n",
    "# Training epochs\n",
    "N_EPOCHS=15\n",
    "\n",
    "# Training batch size\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Dimension of the embedding layer. \n",
    "EMBEDDING_DIM = 8\n",
    "\n",
    "# Metrics to meature training performance\n",
    "METRICS = ['loss', 'binary_accuracy', 'recall', 'precision', 'f1_score']\n",
    "\n",
    "# Folder to store TF callback logs\n",
    "TENSORBOARD_CALLBACK = tf.keras.callbacks.TensorBoard(log_dir='logs')\n",
    "\n",
    "# Verbosity: 0 = silent, 1 = progress bar, 2 = one line per epoch\n",
    "VERBOSITY = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS = {}\n",
    "\n",
    "for dataset, paths in DATASET_PATHS.items():\n",
    "    data = []\n",
    "    labels = []\n",
    "    \n",
    "    # Load dataset metadocs\n",
    "    with open(paths[0], newline='') as datafile:\n",
    "        data_reader = csv.reader(datafile, delimiter='\\n')\n",
    "\n",
    "        for row in data_reader:\n",
    "            data.append(row[0])\n",
    "            \n",
    "    # Load dataset labels\n",
    "    with open(paths[1], newline='') as labelfile:\n",
    "        label_reader = csv.reader(labelfile, delimiter='\\n')\n",
    "\n",
    "        for row in label_reader:\n",
    "            labels.append(int(row[0]))\n",
    "            \n",
    "    DATASETS[dataset] = (data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_CT_SEEDS = set()\n",
    "RANDOM_CV_SEEDS = set()\n",
    "\n",
    "# Generate list of unique random seeds to use with StratifiedShuffleSplit objects\n",
    "while len(RANDOM_CT_SEEDS) < (len(DATASETS.keys()) * (len(DATASETS.keys())-1) * N_TRIALS):\n",
    "    RANDOM_CT_SEEDS.add(np.random.randint(10000))\n",
    "\n",
    "# Generate list of unique random seeds to use with StratifiedShuffleSplit objects\n",
    "while len(RANDOM_CV_SEEDS) < N_TRIALS:\n",
    "    RANDOM_CV_SEEDS.add(np.random.randint(1000))\n",
    "\n",
    "RANDOM_CT_SEEDS = iter(list(RANDOM_CT_SEEDS))\n",
    "RANDOM_CV_SEEDS = list(RANDOM_CV_SEEDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the text vectorization layer to normalize, split, and map strings to integers. \n",
    "vectorize_layer = TextVectorization()\n",
    "    #output_mode='int',\n",
    "    #output_sequence_length=SEQUENCE_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for primary_set, data_labels in DATASETS.items():\n",
    "    for cross_train_set, ct_data_labels in DATASETS.items():\n",
    "        if primary_set == cross_train_set:\n",
    "            pass\n",
    "        else:\n",
    "            print('\\n\\n*************************', primary_set.upper(), '+', cross_train_set.upper(), '*************************')\n",
    "            \n",
    "            # Store metric averages for each trial\n",
    "            train_averages = dict([(metric,[]) for metric in METRICS])\n",
    "            validation_averages = dict([(metric,[]) for metric in METRICS])\n",
    "            ct_averages = dict([(metric,[]) for metric in METRICS])\n",
    "\n",
    "            # Peform N_TRIALS of N_FOLDS CV\n",
    "            for i,RANDOM_SEED in enumerate(RANDOM_CV_SEEDS):\n",
    "                print('\\n\\n******************** TRIAL %d ********************' %(i+1))\n",
    "                \n",
    "                training_data, labels, ct_test_data, ct_test_labels = mix_data(data_labels, ct_data_labels, CROSS_TRAIN_HOLDOUT_RATIO, next(RANDOM_CT_SEEDS))\n",
    "\n",
    "                # Convert data and labels to numpy arrays for training and testing\n",
    "                training_data = np.array(training_data, dtype=object)\n",
    "                labels = np.array(labels)\n",
    "                \n",
    "                k=1 # Fold counter\n",
    "                # Store metric averages for each fold of a single trial\n",
    "                train_history = dict([(metric,[]) for metric in METRICS])\n",
    "                validation_history = dict([(metric,[]) for metric in METRICS])\n",
    "                ct_history = dict([(metric,[]) for metric in METRICS])\n",
    "                \n",
    "                skf = StratifiedShuffleSplit(n_splits=N_FOLDS, train_size=TRAIN_TEST_SPLIT, random_state=RANDOM_SEED)\n",
    "\n",
    "                \n",
    "                for train, test in skf.split(training_data, labels):\n",
    "\n",
    "                    # This will cause the model to build an index of strings to integers.\n",
    "                    # Per TF: It's important to only use training data when creating vocabulary (using the test set would leak information).\n",
    "                    vectorize_layer.set_vocabulary(utils.get_vocabulary(training_data[train]))\n",
    "                    input_dim = len(vectorize_layer.get_vocabulary())\n",
    "\n",
    "                    # Embed vocabulary into embedding_dim dimensions.\n",
    "                    # Embedding tutorial uses size, Text Classification tutorial uses size + 1 for input_dim\n",
    "                    embedding_layer = tf.keras.layers.Embedding(input_dim, EMBEDDING_DIM, name='embedding')\n",
    "\n",
    "                    # Define model structure\n",
    "                    model = Sequential([\n",
    "                        vectorize_layer,\n",
    "                        embedding_layer,\n",
    "                        #Dropout(0.2),\n",
    "                        GlobalAveragePooling1D(),\n",
    "                        #Dropout(0.2),\n",
    "                        Dense(16, activation='relu'),\n",
    "                        Dense(1, activation='sigmoid')\n",
    "                    ])\n",
    "\n",
    "                    # Create model\n",
    "                    model.compile(optimizer='adam',\n",
    "                              loss=tf.keras.losses.BinaryCrossentropy(from_logits=False), # tutorials use true for training, false for production\n",
    "                              metrics=[\n",
    "                                  tf.metrics.BinaryAccuracy(threshold=0.5),\n",
    "                                  tf.keras.metrics.Recall(),\n",
    "                                  tf.keras.metrics.Precision(),\n",
    "                                  F1Score(1, threshold=0.5)\n",
    "                              ]\n",
    "                    )\n",
    "\n",
    "                    print('\\n\\n*************** FOLD %d ***************' %k)\n",
    "\n",
    "                    print('\\n******* TRAIN *******')\n",
    "                    # Train model\n",
    "                    train_results = model.fit(\n",
    "                        training_data[train],\n",
    "                        labels[train],\n",
    "                        batch_size=BATCH_SIZE, \n",
    "                        epochs=N_EPOCHS,\n",
    "                        callbacks=[TENSORBOARD_CALLBACK],\n",
    "                        verbose=VERBOSITY\n",
    "                    )\n",
    "\n",
    "                    print('\\n******* VALIDATION *******')\n",
    "                    # Test model with validation data\n",
    "                    validation_results = model.evaluate(\n",
    "                        training_data[test],\n",
    "                        labels[test],\n",
    "                        callbacks=[TENSORBOARD_CALLBACK],\n",
    "                        return_dict=True,\n",
    "                        verbose=VERBOSITY\n",
    "                    )\n",
    "\n",
    "                    print('\\n******* CT TEST *******')\n",
    "                    # Test model with cross train data\n",
    "                    ct_results = model.evaluate(\n",
    "                        ct_test_data,\n",
    "                        ct_test_labels,\n",
    "                        callbacks=[TENSORBOARD_CALLBACK],\n",
    "                        return_dict=True,\n",
    "                        verbose=VERBOSITY\n",
    "                    )\n",
    "                    \n",
    "                    train_history = save_results(train_results.history, train_history)\n",
    "                    validation_history = save_results(validation_results, validation_history)\n",
    "                    ct_history = save_results(ct_results, ct_history)\n",
    "\n",
    "                    # If we are in the last fold of the trial, average the metric results \n",
    "                    # across all n folds and append to trial_averages\n",
    "                    if k == N_FOLDS:\n",
    "                        for metric, results in train_history.items():\n",
    "                            train_averages[metric].append(sum(results)/len(results))\n",
    "                        for metric, results in validation_history.items():\n",
    "                            validation_averages[metric].append(sum(results)/len(results))\n",
    "                        for metric, results in ct_history.items():\n",
    "                            ct_averages[metric].append(sum(results)/len(results))\n",
    "\n",
    "                    k += 1\n",
    "\n",
    "            RESULTS_FILE = 'trials/'+primary_set+'_'+str(CROSS_TRAIN_RATIO)+'_'+cross_train_set+'_'+str(N_TRIALS)+'_T_'+ str(N_FOLDS)+'_fCV.xlsx'\n",
    "            \n",
    "            '''\n",
    "            Write all results to an excel file.\n",
    "            The first sheet shows metric averages for each trial. The second sheet contains the averages across all trials.\n",
    "            '''\n",
    "            trial_table, averages_table = format_output(train_averages)\n",
    "            \n",
    "            with pd.ExcelWriter(RESULTS_FILE) as writer:\n",
    "                trial_table.to_excel(writer, sheet_name='Training Trials')\n",
    "\n",
    "            with pd.ExcelWriter(RESULTS_FILE, mode='a') as writer:\n",
    "                averages_table.iloc[0].to_excel(writer, sheet_name='Training Averages', header=False)\n",
    "                \n",
    "            trial_table, averages_table = format_output(validation_averages)\n",
    "            \n",
    "            with pd.ExcelWriter(RESULTS_FILE, mode='a') as writer:\n",
    "                trial_table.to_excel(writer, sheet_name='Validation Trials')\n",
    "\n",
    "            with pd.ExcelWriter(RESULTS_FILE, mode='a') as writer:\n",
    "                averages_table.iloc[0].to_excel(writer, sheet_name='Validation Averages', header=False)\n",
    "                \n",
    "            trial_table, averages_table = format_output(ct_averages)\n",
    "            \n",
    "            with pd.ExcelWriter(RESULTS_FILE, mode='a') as writer:\n",
    "                trial_table.to_excel(writer, sheet_name='CT Trials')\n",
    "\n",
    "            with pd.ExcelWriter(RESULTS_FILE, mode='a') as writer:\n",
    "                averages_table.iloc[0].to_excel(writer, sheet_name='CT Averages', header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display model information\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the trained word embeddings\n",
    "weights = model.get_layer('embedding').get_weights()[0]\n",
    "vocab = vectorize_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save embeddings to disk\n",
    "out_vec = io.open(OUTPUT_VECTOR_FILE, 'w', encoding='utf-8')\n",
    "out_meta = io.open(OUTPUT_METADATA_FILE, 'w', encoding='utf-8')\n",
    "\n",
    "for index, word in enumerate(vocab):\n",
    "    if  index == 0: continue # skip 0, it's padding.\n",
    "    vec = weights[index] \n",
    "    out_vec.write('\\t'.join([str(x) for x in vec]) + '\\n')\n",
    "    out_meta.write(word + '\\n')\n",
    "    \n",
    "out_vec.close()\n",
    "out_meta.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Project_2",
   "language": "python",
   "name": "project_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
