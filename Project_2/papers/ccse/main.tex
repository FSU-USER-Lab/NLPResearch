\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=1.0in]{geometry}

\title{Automating Software Traceability Link Recovery and Maintenance using Word Embeddings within a Shallow Neural Network}
\author{Marlan McInnes-Taylor\\Faculty Mentor: Dr. Chris Mills\\Florida State University Department of Computer Science\\1017 Academic Way, Tallahassee, FL 32304\\mcinnest@cs.fsu.edu, crmills@fsu.edu}

\date{}

\usepackage{natbib}
\usepackage{graphicx}

\begin{document}

\maketitle

\section{Abstract}
In addition to code, software systems contain a multitude of files documenting features, known issues, legal requirements, etc. Software traceability is the ability to link related documents from these various sets through a process called \textit{traceability link recovery}. While previous research has shown that established software traceability improves the quality of projects and makes them easier to maintain, establishing software traceability is often a manual, arduous, and error prone task. This research explores automating the process of software traceability link recovery using established techniques in machine learning. The results demonstrate that even with minimally tuned hyperparameters a shallow neural network can effectively predict which text-based artifacts within a software system are related to one another.

\section{Introduction}
    Previous studies have shown that access to information explaining relationships between artifacts in a system leads to higher quality software and lower bug counts. This is largely credited to such information improving common software engineering tasks such as program comprehension, concept and bug localization, and defect prediction among many others \cite{bouillon2013survey, mader2015developers, mader2016preventing}. Unfortunately, the aquisition of traceability information is difficult and typically an afterthought during system construction. Consequently, hundreds or thousands of man hours can be spent after initial development manually infering relationships between artifacts that are constantly in flux as the system changes during development and manintenace \cite{james1997automatic, weidenhaupt1998scenarios, antoniol2000traceability}. Therefore, even if a firm is able to establish traceability for a mission critical system, the effectiveness of that information is potentially only temporary. To address this situation, previous studies have attempted to either completely or partially automate the process of establishing traceability links between system components \cite{borg2014recovering}. In this work, we continue that research agenda by using neural networks to model \textit{potential} links between text-based software artifacts and predict which are valid (i.e., two related documents) and which are invalid (i.e., two unrelated documents). Preliminary results of using this technique are promising and show higher recall and precision than its contemporaries \cite{mills2018automatic, mills2019tracing}.

\section{Materials and Methods}
    The training and test data were comprised of six datasets commonly used to validate approaches for automating traceability link recovery: Albergate, Eanci, eTour, iTrust, Modis, and SMOS. Together, these datasets represent XYZ documents and XYZ potential links, XYZ\% of which are valid and XYZ\% are invalid. Table ABC shows a breakdown of the data by project. Each of the projects involved in our evaluation is written in either Java or C++. The code used for parsing and cleaning the software artifacts is written in Python. TensorFlow 2.0 was used to train and evaluate the neural networks. 
    
    %INSERT TABLE HERE
    
    
\subsection{Methodology}
	Our evaluation consists of a preprocessing, training, and validation phase.
	
\subsubsection{Preprocessing}
    Document text was first tokenized using NLTK's\citep{BirdKleinLoper09} punkt tokenizer. Then documents were cleaned by removing stopwords using NLTK's built in stopwords list augmented with a collection of Java and C++ keywords. Additionally, whitespace, punctuation, and purely numeric tokens were removed from each document. Finally, a Porter Stemmer CITATION was applied to all remaining terms. It is important to note that this stemmer was chosen because it language invariant and our datasets contain both English and Italian words. Each dataset consists of two sets of artifacts of different types (e.g., code classes and use cases), and this same process was applied to documents in both of those sets. 
    
    Once the data was cleaned, we created \textit{metadocuments} by concatenating all possible (order invariant) pairs of documnents such that the documents in a pair belong to different sets of artifacts. Therefore, each \textit{metadocument} contains data from a unique pair of potentially related documents in the system, each of these pairs is then a potential traceability link by construction. Further, the text of each metadocument was randomly shuffled using the Numpy's\citep{harris2020array} shuffle method, in order to reduce potential bias from token arrangement when setting the sequence length. Finally, each metadocument in our aggregated dataset was labeled as either a valid link between two related documents or an invalid link between two unrelated ones as specified in each dataset's oracle file. This information was then used to evaluate model predictions.

    As shown in Table ABC, class imbalance was present in all datasets due to the large number of metadocuments (i.e., artifact pairs). As one might expect, for a given system there are far fewer metadaocuments representing valid links than invalid ones. To mitigate negative effects of class imbalance on model performance, each dataset was balanced using Imbalanced-Learn's\citep{JMLR:v18:16-365} SMOTE implementation. For our evaluation, we continued synthetic metadocument generation until an equal number of valid and invalid links was reached within each dataset.

\subsubsection{Training and Validation}
    In order to construct a minimally complex proof of concept for the approach, a shallow Tensorflow\citep{abadi2016tensorflow} model was implemented classify unlabeled metadocuments as valid or invalid. The model's first two layers perform text vectorization and transform the vectors using word embeddings. These vectors are then pooled before being passed into a 16 node dense layer followed by the output layer. Based on initial positive results on the SMOS dataset, we kept these hyperparameters and performed 50 trials of 10 fold cross validation with 15 epochs per fold on each dataset. Shuffled stratification via scikit-learn's\citep{sklearn_api} StratifiedShuffleSplit implementation was used to sample the dataset in each fold to again minimize selection bias. 

\section{Results}
\label{T:results}
    \begin{center}
        \begin{tabular}{ |c|c|c|c|c|c| } 
         \hline
         \textbf{Dataset} & \textbf{Loss} & \textbf{Accuracy} & \textbf{Recall} & \textbf{Precision} & \textbf{F1Score} \\ 
         \hline
         Albergate & 0.28 & 0.95 & 0.91 & 0.98 & 0.94 \\ 
         \hline
         Eanci & 0.11 & 0.96 & 0.94 & 0.98 & 0.96 \\ 
         \hline
         Etour & 0.13 & 0.96 & 0.93 & 0.99 & 0.96 \\ 
         \hline
         iTrust & 0.17 & 0.97 & 0.95 & 0.99 & 0.97 \\ 
         \hline
         Modis & 0.29 & 0.93 & 0.94 & 0.92 & 0.93 \\ 
         \hline
         SMOS & 0.33 & 0.87 & 0.75 & 0.98 & 0.85 \\ 
         \hline
        \end{tabular}
    \end{center}
    
\section{Conclusions and Future Work}
    Table \ref{T:results} shows the results of our evaluation. Note that here we report recall, precision, and F1 score in addition to accuracy. This is because accuracy can be artificially inflated in imbalanced data. For example, in a dataset of 100 samples with 1 ``false" label, a machine that always predicts ``true" has high accuracy, but it's low recall illustrates the machines impracticality. Therefore, given our precision and recall results, it is clear that word embeddings used within a shallow network can successfully perform metadocument classification for the systems under study. Future work will focus on improving this model by further optimizing its hyperparameters and evaluating the model's generalizability. Moreover, we plan to investigate minimizing the start-up cost associated with employing our model to make it appealing to industry partners with greenfield systems. Initially, we plan to perform a series of experiments to identify the minimum data requirements using supportive techniques such as Active Learning and cross training. 
    
\newpage

\bibliographystyle{plain}
\bibliography{references}
\end{document}
