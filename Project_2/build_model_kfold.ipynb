{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import utils\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from tensorflow.keras import Model, Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense, Dropout, Embedding, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from tensorflow_addons.metrics import F1Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DATA_FILE = 'data/smos/smos_data_porter_balanced.txt'\n",
    "INPUT_LABEL_FILE = 'data/smos/smos_labels_porter_balanced.txt'\n",
    "\n",
    "N_FOLDS = 10 # Number of cross validation folds. Default: 10\n",
    "N_TRIALS = 50 # Number of trials of n fold cv\n",
    "\n",
    "# Only edit first chunk of path\n",
    "RESULTS_FILE = 'data/smos/smos_' + str(N_TRIALS) + '_TRIALS_' + str(N_FOLDS) + '_FOLD_CV.xlsx'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Ratio defining how much data is reserved for testing. Ex: 0.8 is an 80/20 train/test split\n",
    "Float on interval (0.0, 1.0)\n",
    "'''\n",
    "TRAIN_TEST_SPLIT = 0.8 \n",
    "\n",
    "'''\n",
    "Number of words in a sequence.\n",
    "Note: if using a balanced set, we have already set a seq len, \n",
    "so the max len (obtained below) should be used for SEQUENCE_LENGTH\n",
    "'''\n",
    "SEQUENCE_LENGTH = 220\n",
    "\n",
    "# Training batch size\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Folder to store TF callback logs\n",
    "TENSORBOARD_CALLBACK = tf.keras.callbacks.TensorBoard(log_dir='logs')\n",
    "\n",
    "# Dimension of the embedding layer. \n",
    "EMBEDDING_DIM = 8\n",
    "\n",
    "# Metrics to meature training performance\n",
    "METRICS = ['loss', 'binary_accuracy', 'recall', 'precision', 'f1_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = []\n",
    "labels = []\n",
    "lengths = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset metadocs\n",
    "with open(INPUT_DATA_FILE, newline='') as datafile:\n",
    "    data_reader = csv.reader(datafile, delimiter='\\n')\n",
    "    \n",
    "    for row in data_reader:\n",
    "        training_data.append(row[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset labels\n",
    "with open(INPUT_LABEL_FILE, newline='') as labelfile:\n",
    "    label_reader = csv.reader(labelfile, delimiter='\\n')\n",
    "    \n",
    "    for row in label_reader:\n",
    "        labels.append(int(row[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Sequence information for loaded set\n",
    "Note: if using a balanced set, we have already set a seq len, so the max len should be used for SEQUENCE_LENGTH\n",
    "'''\n",
    "for seq in training_data:\n",
    "    lengths.append(len(seq.split()))\n",
    "\n",
    "print('Number of metadocuments: ', len(training_data))\n",
    "print('Vocab size: ', utils.vocabulary_size(training_data))\n",
    "print('Avg seq length: ', sum(lengths) / len(lengths))\n",
    "print('Min seq len: ', min(lengths))\n",
    "print('Max seq len: ', max(lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data and labels to numpy arrays for training and testing\n",
    "training_data = np.array(training_data, dtype=object)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the text vectorization layer to normalize, split, and map strings to integers. \n",
    "vectorize_layer = TextVectorization(\n",
    "    output_mode='int',\n",
    "    output_sequence_length=SEQUENCE_LENGTH\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEEDS = []\n",
    "\n",
    "# Generate list of unique random seeds to use with StratifiedShuffleSplit objects\n",
    "while len(RANDOM_SEEDS) < N_TRIALS:\n",
    "    seed = np.random.randint(1000)\n",
    "    if seed in RANDOM_SEEDS:\n",
    "        continue\n",
    "    else:\n",
    "        RANDOM_SEEDS.append(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store metric averages for each trial\n",
    "trial_averages = dict([(metric,[]) for metric in METRICS])\n",
    "\n",
    "# Peform N_TRIALS of N_FOLDS CV\n",
    "for i,RANDOM_SEED in enumerate(RANDOM_SEEDS):\n",
    "    k=1 # Fold counter\n",
    "    # Store metric averages for each fold of a single trial\n",
    "    trial_history = dict([(metric,[]) for metric in ['loss', 'binary_accuracy', 'recall', 'precision', 'f1_score']])\n",
    "    skf = StratifiedShuffleSplit(n_splits=N_FOLDS, train_size=TRAIN_TEST_SPLIT, random_state=RANDOM_SEED)\n",
    "\n",
    "    print('\\n\\n******************** TRIAL %d ********************' %(i+1))\n",
    "    for train, test in skf.split(training_data, labels):\n",
    "\n",
    "        # This will cause the model to build an index of strings to integers.\n",
    "        # Per TF: It's important to only use training data when creating vocabulary (using the test set would leak information).\n",
    "        vectorize_layer.set_vocabulary(utils.get_vocabulary(training_data[train]))\n",
    "        input_dim = len(vectorize_layer.get_vocabulary())\n",
    "\n",
    "        # Embed vocabulary into embedding_dim dimensions.\n",
    "        # Embedding tutorial uses size, Text Classification tutorial uses size + 1 for input_dim\n",
    "        embedding_layer = tf.keras.layers.Embedding(input_dim, EMBEDDING_DIM, name='embedding')\n",
    "\n",
    "        # Define model structure\n",
    "        model = Sequential([\n",
    "            vectorize_layer,\n",
    "            embedding_layer,\n",
    "            #Dropout(0.2),\n",
    "            GlobalAveragePooling1D(),\n",
    "            #Dropout(0.2),\n",
    "            Dense(16, activation='relu'),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "\n",
    "        # Create model\n",
    "        model.compile(optimizer='adam',\n",
    "                  loss=tf.keras.losses.BinaryCrossentropy(from_logits=False), # tutorials use true for training, false for production\n",
    "                  metrics=[tf.metrics.BinaryAccuracy(threshold=0.5), tf.keras.metrics.Recall(), tf.keras.metrics.Precision(), F1Score(1, threshold=0.5)]\n",
    "        )\n",
    "\n",
    "        print('\\n\\n*************** FOLD %d ***************' %k)\n",
    "\n",
    "\n",
    "        print('\\n******* TRAIN *******')\n",
    "        # Train model\n",
    "        # Verbosity: 0 = silent, 1 = progress bar, 2 = one line per epoch\n",
    "        history = model.fit(\n",
    "            training_data[train],\n",
    "            labels[train],\n",
    "            batch_size=BATCH_SIZE, \n",
    "            epochs=15,\n",
    "            callbacks=[TENSORBOARD_CALLBACK],\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "        print('\\n******* TEST *******')\n",
    "        # Test model\n",
    "        model.evaluate(training_data[test], labels[test])\n",
    "\n",
    "        # Append current fold results to trial_history dict\n",
    "        # Metric names are appended with a _(run number) each trial, hence the nested for loop\n",
    "        for current_metric, results in history.history.items():\n",
    "            for metric in trial_history.keys():\n",
    "                if metric in current_metric:\n",
    "                    # F1Score stores results as a list of lists instead of list of floats\n",
    "                    if metric == 'f1_score':\n",
    "                        total=0\n",
    "                        for result in results:\n",
    "                            total+=result[0]\n",
    "                        trial_history[metric].append(total/len(results))\n",
    "                    else:\n",
    "                        trial_history[metric].append(sum(results)/len(results))\n",
    "\n",
    "                    break\n",
    "\n",
    "        # If we are in the last fold of the trial, average the metric results \n",
    "        # across all n folds and append to trial_averages\n",
    "        if k == N_FOLDS:\n",
    "            for metric, results in trial_history.items():\n",
    "                trial_averages[metric].append(sum(results)/len(results))\n",
    "\n",
    "        k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_averages = {}\n",
    "\n",
    "# Stores metric averages across all trials\n",
    "for metric, results in trial_averages.items():\n",
    "    result_averages[metric+'_avg'] = sum(results)/len(results)\n",
    "    \n",
    "trial_table = pd.DataFrame(trial_averages)\n",
    "averages_table = pd.DataFrame(result_averages, index=result_averages.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Write all results to an excel file.\n",
    "The first sheet shows metric averages for each trial. The second sheet contains the averages across all trials. \n",
    "'''\n",
    "with pd.ExcelWriter(RESULTS_FILE) as writer:\n",
    "    trial_table.to_excel(writer, sheet_name='Trials')\n",
    "    \n",
    "with pd.ExcelWriter(RESULTS_FILE, mode='a') as writer:\n",
    "    averages_table.iloc[0].to_excel(writer, sheet_name='Averages', header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display model information\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Retrieve the trained word embeddings\n",
    "weights = model.get_layer('embedding').get_weights()[0]\n",
    "vocab = vectorize_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Save embeddings to disk\n",
    "out_vec = io.open('data/smos/smos_porter_balanced_vectors.tsv', 'w', encoding='utf-8')\n",
    "out_meta = io.open('data/smos/smos_porter_balanced_metadata.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for index, word in enumerate(vocab):\n",
    "    if  index == 0: continue # skip 0, it's padding.\n",
    "    vec = weights[index] \n",
    "    out_vec.write('\\t'.join([str(x) for x in vec]) + '\\n')\n",
    "    out_meta.write(word + '\\n')\n",
    "    \n",
    "out_vec.close()\n",
    "out_meta.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Project_2",
   "language": "python",
   "name": "project_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
