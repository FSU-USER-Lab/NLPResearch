{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import csv\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import utils\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from tensorflow.keras import Model, Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense, Dropout, Embedding, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from tensorflow_addons.metrics import F1Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = []\n",
    "labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = 'data/smos/smos_data_porter_bal_shuf.txt'\n",
    "label_file = 'data/smos/smos_labels_bal_shuf.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_file, newline='') as datafile:\n",
    "    data_reader = csv.reader(datafile, delimiter='\\n')\n",
    "    \n",
    "    for row in data_reader:\n",
    "        training_data.append(row[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(label_file, newline='') as labelfile:\n",
    "    label_reader = csv.reader(labelfile, delimiter='\\n')\n",
    "    \n",
    "    for row in label_reader:\n",
    "        labels.append(int(row[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = []\n",
    "\n",
    "for seq in training_data:\n",
    "    lengths.append(len(seq.split()))\n",
    "\n",
    "print('Number of metadocuments: ', len(training_data))\n",
    "print('Vocab size: ', utils.vocabulary_size(training_data))\n",
    "print('Avg seq length: ', sum(lengths) / len(lengths))\n",
    "print('Min seq len: ', min(lengths))\n",
    "print('Max seq len: ', max(lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and split dataset\n",
    "full_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    'data/smos/train_balanced',\n",
    "    label_mode='binary',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir='logs')\n",
    "\n",
    "# Convert data and labels to numpy arrays for training\n",
    "training_data = np.array(training_data, dtype=object)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Vocabulary size and number of words in a sequence.\n",
    "If we are using data which has already been balanced then seq len should be set to the max len above\n",
    "since the seq len will have already been set before balancing.\n",
    "'''\n",
    "sequence_length = 200\n",
    "vocab_size = utils.vocabulary_size(training_data)\n",
    "\n",
    "'''\n",
    "Use the text vectorization layer to normalize, split, and map strings to \n",
    "integers. Note that the layer uses the custom standardization defined above. \n",
    "'''\n",
    "vectorize_layer = TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length)\n",
    "\n",
    "# Make a text-only dataset (no labels) and call adapt to build the vocabulary.\n",
    "text_ds = full_ds.map(lambda x, y: x)\n",
    "vectorize_layer.adapt(text_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimension of the embedding layer. \n",
    "embedding_dim = 8\n",
    "\n",
    "# Embed vocabulary into embedding_dim dimensions.\n",
    "# Embedding tutorial uses size, Text Classification tutorial uses size + 1\n",
    "embedding_layer = tf.keras.layers.Embedding(vocab_size + 1, embedding_dim, name='embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedShuffleSplit(n_splits=10, train_size=0.8)\n",
    "i=1\n",
    "\n",
    "for train, test in skf.split(training_data, labels):    \n",
    "    # Define model structure\n",
    "    model = Sequential([\n",
    "        vectorize_layer,\n",
    "        embedding_layer,\n",
    "        #Dropout(0.2),\n",
    "        GlobalAveragePooling1D(),\n",
    "        #Dropout(0.2),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    # Create model\n",
    "    model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=[tf.metrics.BinaryAccuracy(threshold=0.5), tf.keras.metrics.Recall(), tf.keras.metrics.Precision()])\n",
    "    \n",
    "    print('\\n\\n*************** FOLD %d ***************' %i)\n",
    "    i += 1\n",
    "    \n",
    "    print('\\n******* TRAIN *******')\n",
    "    # Train model\n",
    "    model.fit(\n",
    "        training_data[train],\n",
    "        labels[train],\n",
    "        batch_size=batch_size,\n",
    "        #validation_data=val_ds, \n",
    "        epochs=15,\n",
    "        callbacks=[tensorboard_callback]\n",
    "    )\n",
    "    \n",
    "    print('\\n******* TEST *******')\n",
    "    # Test model\n",
    "    model.evaluate(training_data[test], labels[test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    'data/smos/train',\n",
    "    batch_size=batch_size,\n",
    "    label_mode='binary',\n",
    "    seed=seed\n",
    ")\n",
    "\n",
    "original_ds = original_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(original_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the trained word embeddings\n",
    "weights = model.get_layer('embedding').get_weights()[0]\n",
    "vocab = vectorize_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save embeddings to disk\n",
    "out_vec = io.open('data/smos/smos_porter_balanced_vectors.tsv', 'w', encoding='utf-8')\n",
    "out_meta = io.open('data/smos/smos_porter_balanced_metadata.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for index, word in enumerate(vocab):\n",
    "    if  index == 0: continue # skip 0, it's padding.\n",
    "    vec = weights[index] \n",
    "    out_vec.write('\\t'.join([str(x) for x in vec]) + '\\n')\n",
    "    out_meta.write(word + '\\n')\n",
    "    \n",
    "out_vec.close()\n",
    "out_meta.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Project_2",
   "language": "python",
   "name": "project_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
