{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import csv\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import utils\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from tensorflow.keras import Model, Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense, Dropout, Embedding, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from tensorflow_addons.metrics import F1Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DATA_FILE = 'data/smos/smos_data_porter_balanced.txt'\n",
    "INPUT_LABEL_FILE = 'data/smos/smos_labels_porter_balanced.txt'\n",
    "\n",
    "N_FOLDS = 10 # Number of cross validation folds. Default: 10\n",
    "TRAIN_TEST_SPLIT = 0.8 # Float on interval (0.0, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = []\n",
    "labels = []\n",
    "lengths = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(INPUT_DATA_FILE, newline='') as datafile:\n",
    "    data_reader = csv.reader(datafile, delimiter='\\n')\n",
    "    \n",
    "    for row in data_reader:\n",
    "        training_data.append(row[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(INPUT_LABEL_FILE, newline='') as labelfile:\n",
    "    label_reader = csv.reader(labelfile, delimiter='\\n')\n",
    "    \n",
    "    for row in label_reader:\n",
    "        labels.append(int(row[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seq in training_data:\n",
    "    lengths.append(len(seq.split()))\n",
    "\n",
    "print('Number of metadocuments: ', len(training_data))\n",
    "print('Vocab size: ', utils.vocabulary_size(training_data))\n",
    "print('Avg seq length: ', sum(lengths) / len(lengths))\n",
    "print('Min seq len: ', min(lengths))\n",
    "print('Max seq len: ', max(lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data and labels to numpy arrays for training and testing\n",
    "training_data = np.array(training_data, dtype=object)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training batch size\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Folder to store callback logs\n",
    "TENSORBOARD_CALLBACK = tf.keras.callbacks.TensorBoard(log_dir='logs')\n",
    "\n",
    "# Dimension of the embedding layer. \n",
    "EMBEDDING_DIM = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Number of words in a sequence.\n",
    "If we are using data which has already been balanced then seq len should be set to the max len above\n",
    "since the seq len will have already been set before balancing.\n",
    "'''\n",
    "SEQUENCE_LENGTH = 200\n",
    "\n",
    "'''\n",
    "Use the text vectorization layer to normalize, split, and map strings to \n",
    "integers. Note that the layer uses the custom standardization defined above. \n",
    "'''\n",
    "vectorize_layer = TextVectorization(\n",
    "    output_mode='int',\n",
    "    output_sequence_length=SEQUENCE_LENGTH\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedShuffleSplit(n_splits=N_FOLDS, train_size=TRAIN_TEST_SPLIT)\n",
    "i=1\n",
    "\n",
    "for train, test in skf.split(training_data, labels):\n",
    "    \n",
    "    # This will cause the model to build an index of strings to integers.\n",
    "    # Per TF: It's important to only use training data when calling adapt (using the test set would leak information).\n",
    "    #vectorize_layer.adapt(training_data[train]) FIX THIS by using set vocab\n",
    "    vectorize_layer.set_vocabulary(utils.get_vocabulary(training_data[train]))\n",
    "    input_dim = len(vectorize_layer.get_vocabulary())\n",
    "    \n",
    "    # Embed vocabulary into embedding_dim dimensions.\n",
    "    # Embedding tutorial uses size, Text Classification tutorial uses size + 1 for input_dim\n",
    "    embedding_layer = tf.keras.layers.Embedding(input_dim, embedding_dim, name='embedding')\n",
    "    \n",
    "    # Define model structure\n",
    "    model = Sequential([\n",
    "        vectorize_layer,\n",
    "        embedding_layer,\n",
    "        #Dropout(0.2),\n",
    "        GlobalAveragePooling1D(),\n",
    "        #Dropout(0.2),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    # Create model\n",
    "    model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=False), # tutorials use true for training, false for production\n",
    "              metrics=[tf.metrics.BinaryAccuracy(threshold=0.5), tf.keras.metrics.Recall(), tf.keras.metrics.Precision(), F1Score(1, threshold=0.5)]\n",
    "    )\n",
    "    \n",
    "    print('\\n\\n*************** FOLD %d ***************' %i)\n",
    "    i += 1\n",
    "    \n",
    "    print('\\n******* TRAIN *******')\n",
    "    # Train model\n",
    "    model.fit(\n",
    "        training_data[train],\n",
    "        labels[train],\n",
    "        batch_size=batch_size,\n",
    "        #validation_data=val_ds, \n",
    "        epochs=15,\n",
    "        callbacks=[tensorboard_callback]\n",
    "    )\n",
    "    \n",
    "    print('\\n******* TEST *******')\n",
    "    # Test model\n",
    "    model.evaluate(training_data[test], labels[test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Retrieve the trained word embeddings\n",
    "weights = model.get_layer('embedding').get_weights()[0]\n",
    "vocab = vectorize_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Save embeddings to disk\n",
    "out_vec = io.open('data/smos/smos_porter_balanced_vectors.tsv', 'w', encoding='utf-8')\n",
    "out_meta = io.open('data/smos/smos_porter_balanced_metadata.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for index, word in enumerate(vocab):\n",
    "    if  index == 0: continue # skip 0, it's padding.\n",
    "    vec = weights[index] \n",
    "    out_vec.write('\\t'.join([str(x) for x in vec]) + '\\n')\n",
    "    out_meta.write(word + '\\n')\n",
    "    \n",
    "out_vec.close()\n",
    "out_meta.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Project_2",
   "language": "python",
   "name": "project_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
