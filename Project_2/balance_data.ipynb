{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import utils\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = [] # list of strings\n",
    "filename = 'data/smos/smos_data_porter.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(filename, newline='') as datafile:\n",
    "    data_reader = csv.reader(datafile, delimiter='\\n')\n",
    "    \n",
    "    for row in data_reader:\n",
    "        training_data.append(row[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = []\n",
    "\n",
    "for seq in training_data:\n",
    "    lengths.append(len(seq.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [] # list of strings\n",
    "filename = 'data/smos/smos_labels.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(filename, newline='') as datafile:\n",
    "    data_reader = csv.reader(datafile, delimiter='\\n')\n",
    "    \n",
    "    for row in data_reader:\n",
    "        labels.append(int(row[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Vocab size: ', utils.vocabulary_size(training_data))\n",
    "print('Avg seq length: ', sum(lengths) / len(lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and split dataset\n",
    "batch_size = 32\n",
    "seed = 123\n",
    "\n",
    "full_ds = tf.keras.preprocessing.text_dataset_from_directory('data/smos/train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text_batch, label_batch in train_ds.take(1):\n",
    "    for i in range(2):\n",
    "        print(label_batch[i].numpy(), text_batch.numpy()[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary size and number of words in a sequence.\n",
    "# Using ~avg sequence length of all sequences\n",
    "sequence_length = 200\n",
    "vocab_size = utils.vocabulary_size(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the text vectorization layer to normalize, split, and map strings to \n",
    "# integers. Note that the layer uses the custom standardization defined above. \n",
    "# Set maximum_sequence length as all samples are not of the same length.\n",
    "# size + 1 for UNK\n",
    "vectorize_layer = TextVectorization(\n",
    "    max_tokens=vocab_size + 1,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length)\n",
    "\n",
    "# Make a text-only dataset (no labels) and call adapt to build the vocabulary.\n",
    "text_ds = full_ds.map(lambda x, y: x)\n",
    "vectorize_layer.adapt(text_ds) # we should build our own vocab instead, per the tf documentation, how do we account for managled words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vocab = vocab = vectorize_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set()\n",
    "\n",
    "for string in training_data:\n",
    "        token_list = string.split()\n",
    "        for token in token_list:\n",
    "            vocab.add(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(tf_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print words which were magled by fixed sequence length\n",
    "# what happens when we fix sequence length and that chops a word?\n",
    "inverse_vocab = {}\n",
    "\n",
    "for i, word in enumerate(tf_vocab):\n",
    "    inverse_vocab[i] = word\n",
    "    if word not in vocab:\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inverse_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_data = np.zeros((len(training_data), sequence_length))\n",
    "vectorized_labels = np.zeros(len(training_data))\n",
    "j = 0\n",
    "\n",
    "for batch in full_ds:\n",
    "    for sequence, label in zip(vectorize_layer(batch[0]), batch[1]):\n",
    "        vectorized_data[j] = sequence\n",
    "        vectorized_labels[j] = label \n",
    "        j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check vectors for [UNK] in sequence\n",
    "for row in vectorized_data:\n",
    "    for val in row:\n",
    "        if val == 1:\n",
    "            print('hit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_data_enc, balanced_labels = SMOTE(sampling_strategy=.75).fit_resample(vectorized_data, vectorized_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/smos/smos_filenames_balanced.txt', 'w', newline='') as fnfile:\n",
    "        filename_writer = csv.writer(fnfile, quoting=csv.QUOTE_MINIMAL)\n",
    "\n",
    "        with open('data/smos/smos_data_porter_balanced.txt', 'w', newline='') as datafile:\n",
    "            data_writer = csv.writer(datafile, quoting=csv.QUOTE_MINIMAL)\n",
    "\n",
    "            with open('data/smos/smos_labels_balanced.txt', 'w', newline='') as labelfile:\n",
    "                label_writer = csv.writer(labelfile, quoting=csv.QUOTE_MINIMAL)  \n",
    "\n",
    "                i = 0\n",
    "                for row, label in zip(balanced_data_enc, balanced_labels):\n",
    "                    decoded = []\n",
    "                    for val in row:\n",
    "                        decoded.append(inverse_vocab[int(val)])\n",
    "                        \n",
    "                    filename_writer.writerow(['file'+str(i)])\n",
    "                    # we will get double spaces due to a 0 being mapped to '' \n",
    "                    data_writer.writerow([' '.join(decoded)])\n",
    "                    label_writer.writerow([int(label)])\n",
    "                    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Project_2",
   "language": "python",
   "name": "project_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
