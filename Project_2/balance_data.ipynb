{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook to perform SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import utils\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DATA_FILE = 'data/smos/smos_data_porter.txt'\n",
    "INPUT_LABEL_FILE = 'data/smos/smos_labels_porter.txt'\n",
    "\n",
    "OUTPUT_DATA_FILE = 'data/smos/smos_data_balanced.txt'\n",
    "OUTPUT_LABEL_FILE = 'data/smos/smos_labels_balanced.txt'\n",
    "\n",
    "BALANCE_RATIO = 1.0 # Float on interval (0.0, 1.0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = []\n",
    "labels = []\n",
    "lengths = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(INPUT_DATA_FILE, newline='') as datafile:\n",
    "    data_reader = csv.reader(datafile, delimiter='\\n')\n",
    "    \n",
    "    for row in data_reader:\n",
    "        training_data.append(row[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(INPUT_LABEL_FILE, newline='') as labelfile:\n",
    "    label_reader = csv.reader(labelfile, delimiter='\\n')\n",
    "    \n",
    "    for row in label_reader:\n",
    "        labels.append(int(row[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seq in training_data:\n",
    "    lengths.append(len(seq.split()))\n",
    "\n",
    "print('Number of metadocuments: ', len(training_data))\n",
    "print('Vocab size: ', utils.vocabulary_size(training_data))\n",
    "print('Avg seq length: ', sum(lengths) / len(lengths))\n",
    "print('Min seq len: ', min(lengths))\n",
    "print('Max seq len: ', max(lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the sequence length of each metadocument in the dataset\n",
    "X = [x for x in range(len(lengths))]\n",
    "\n",
    "plt.scatter(X, lengths)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data and labels to TF dataset in order to use TextVectorization\n",
    "unbalanced_data = [tf.convert_to_tensor(metadoc) for metadoc in training_data]\n",
    "unbalanced_ds = tf.data.Dataset.from_tensor_slices((unbalanced_data,labels)).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify number of words in a sequence for vectorize layer\n",
    "# Using ~avg sequence length of all sequences \n",
    "SEQUENCE_LENGTH = 220\n",
    "\n",
    "# Use the text vectorization layer to split, prune and map strings to \n",
    "# integers. Note that the layer uses the custom standardization defined above. \n",
    "# Set maximum_sequence length as all samples are not of the same length.\n",
    "# size + 1 for UNK\n",
    "vectorize_layer = TextVectorization(\n",
    "    output_mode='int',\n",
    "    output_sequence_length=SEQUENCE_LENGTH)\n",
    "\n",
    "# Load vocab into vectorization layer\n",
    "vectorize_layer.set_vocabulary(utils.get_vocabulary(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List structure\n",
    "tf_vocab = vectorize_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get original vocabulary from unbalanced training data\n",
    "vocab = utils.get_vocabulary(training_data)\n",
    "inverse_vocab = {}\n",
    "\n",
    "# Create an inverse vocabulary so we can decode the balanced vectorized data\n",
    "# Index of word in vectorization layer's vocabulary maps to it's int encoding\n",
    "for i, word in enumerate(tf_vocab):\n",
    "    inverse_vocab[i] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print any words added to vocab by TF\n",
    "for word in enumerate(tf_vocab):    \n",
    "    if word[1] not in vocab:\n",
    "        if word[1] == '':\n",
    "            print('empty string')\n",
    "        else:\n",
    "            print(word[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inverse_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create np arrays to store vectorized data\n",
    "vectorized_data = np.zeros((len(training_data), SEQUENCE_LENGTH))\n",
    "vectorized_labels = np.zeros(len(training_data))\n",
    "i = 0\n",
    "\n",
    "# Vectorize data and arrays for data balancing\n",
    "for batch in unbalanced_ds:\n",
    "    for sequence, label in zip(vectorize_layer(batch[0]), batch[1]):\n",
    "        vectorized_data[i] = sequence\n",
    "        vectorized_labels[i] = label \n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check vectors for [UNK]  and empty string in sequence\n",
    "num_unk = 0\n",
    "num_empty = 0\n",
    "\n",
    "for row in vectorized_data:\n",
    "    for val in row:\n",
    "        if val == 0:\n",
    "            num_empty += 1\n",
    "        elif val == 1:\n",
    "            num_unk += 1\n",
    "            \n",
    "print('Percent empty tokens: %f' %((num_empty/(len(training_data * SEQUENCE_LENGTH)))*100))\n",
    "print('Percent unk tokens: %f' %((num_unk/(len(training_data * SEQUENCE_LENGTH)))*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_data_enc, balanced_labels = SMOTE(sampling_strategy=BALANCE_RATIO).fit_resample(vectorized_data, vectorized_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new data and label files for balanced set\n",
    "with open(OUTPUT_DATA_FILE, 'w', newline='') as datafile:\n",
    "    data_writer = csv.writer(datafile, quoting=csv.QUOTE_MINIMAL)\n",
    "\n",
    "    with open(OUTPUT_LABEL_FILE, 'w', newline='') as labelfile:\n",
    "        label_writer = csv.writer(labelfile, quoting=csv.QUOTE_MINIMAL)  \n",
    "\n",
    "        i = 0\n",
    "        for row, label in zip(balanced_data_enc, balanced_labels):\n",
    "            decoded = []\n",
    "            for val in row:\n",
    "                decoded.append(inverse_vocab[int(val)])\n",
    "                \n",
    "            # we will get double spaces due to a 0 being mapped to '' \n",
    "            data_writer.writerow([' '.join(decoded)])\n",
    "            label_writer.writerow([int(label)])\n",
    "            i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Project_2",
   "language": "python",
   "name": "project_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
