{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import utils\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = []\n",
    "labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = 'data/smos/smos_data_porter_shuffled.txt'\n",
    "label_file = 'data/smos/smos_labels.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_file, newline='') as datafile:\n",
    "    data_reader = csv.reader(datafile, delimiter='\\n')\n",
    "    \n",
    "    for row in data_reader:\n",
    "        training_data.append(row[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(label_file, newline='') as labelfile:\n",
    "    label_reader = csv.reader(labelfile, delimiter='\\n')\n",
    "    \n",
    "    for row in label_reader:\n",
    "        labels.append(int(row[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = []\n",
    "\n",
    "for seq in training_data:\n",
    "    lengths.append(len(seq.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of metadocuments: ', len(training_data))\n",
    "print('Vocab size: ', utils.vocabulary_size(training_data))\n",
    "print('Avg seq length: ', sum(lengths) / len(lengths))\n",
    "print('Min seq len: ', min(lengths))\n",
    "print('Max seq len: ', max(lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots the sequence length of each metadocument in the dataset\n",
    "X = [x for x in range(len(lengths))]\n",
    "\n",
    "plt.scatter(X, lengths)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and split dataset\n",
    "batch_size = 32\n",
    "seed = 123\n",
    "\n",
    "full_ds = tf.keras.preprocessing.text_dataset_from_directory('data/smos/train_s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify vocabulary size and number of words in a sequence for vectorize layer\n",
    "# Using ~avg sequence length of all sequences\n",
    "sequence_length = 200\n",
    "vocab_size = utils.vocabulary_size(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the text vectorization layer to split, prune and map strings to \n",
    "# integers. Note that the layer uses the custom standardization defined above. \n",
    "# Set maximum_sequence length as all samples are not of the same length.\n",
    "# size + 1 for UNK\n",
    "vectorize_layer = TextVectorization(\n",
    "    max_tokens=vocab_size + 1,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length)\n",
    "\n",
    "# Make a text-only dataset (no labels) and call adapt to build the vocabulary.\n",
    "text_ds = full_ds.map(lambda x, y: x)\n",
    "# Per the tf documentation, we should build our own vocab instead, how do we account for managled words?\n",
    "vectorize_layer.adapt(text_ds) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vocab = vectorize_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(tf_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get original vocabulary from unbalanced training data\n",
    "vocab = (utils.get_vocabulary(training_data)).keys()\n",
    "inverse_vocab = {}\n",
    "\n",
    "# Create an inverse vocabulary so we can decode the balanced vectorized data\n",
    "for i, word in enumerate(tf_vocab):\n",
    "    inverse_vocab[i] = word\n",
    "    if word not in vocab:\n",
    "        print(word) # print words which were magled by fixed sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inverse_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_data = np.zeros((len(training_data), sequence_length))\n",
    "vectorized_labels = np.zeros(len(training_data))\n",
    "i = 0\n",
    "\n",
    "# Build np arrays for data balancing\n",
    "for batch in full_ds:\n",
    "    for sequence, label in zip(vectorize_layer(batch[0]), batch[1]):\n",
    "        vectorized_data[i] = sequence\n",
    "        vectorized_labels[i] = label \n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check vectors for [UNK] in sequence\n",
    "for row in vectorized_data:\n",
    "    for val in row:\n",
    "        if val == 1:\n",
    "            print('hit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_data_enc, balanced_labels = SMOTE(sampling_strategy=.75).fit_resample(vectorized_data, vectorized_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new data, label, and filename files for balanced set\n",
    "with open('data/smos/smos_filenames_bal_shuf.txt', 'w', newline='') as fnfile:\n",
    "        filename_writer = csv.writer(fnfile, quoting=csv.QUOTE_MINIMAL)\n",
    "\n",
    "        with open('data/smos/smos_data_porter_bal_shuf.txt', 'w', newline='') as datafile:\n",
    "            data_writer = csv.writer(datafile, quoting=csv.QUOTE_MINIMAL)\n",
    "\n",
    "            with open('data/smos/smos_labels_bal_shuf.txt', 'w', newline='') as labelfile:\n",
    "                label_writer = csv.writer(labelfile, quoting=csv.QUOTE_MINIMAL)  \n",
    "\n",
    "                i = 0\n",
    "                for row, label in zip(balanced_data_enc, balanced_labels):\n",
    "                    decoded = []\n",
    "                    for val in row:\n",
    "                        decoded.append(inverse_vocab[int(val)])\n",
    "                        \n",
    "                    filename_writer.writerow(['file'+str(i)])\n",
    "                    # we will get double spaces due to a 0 being mapped to '' \n",
    "                    data_writer.writerow([' '.join(decoded)])\n",
    "                    label_writer.writerow([int(label)])\n",
    "                    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Project_2",
   "language": "python",
   "name": "project_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
