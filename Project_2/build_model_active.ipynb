{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interested-optimum",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "#import io\n",
    "#import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "#import pandas as pd\n",
    "import tensorflow as tf\n",
    "import utils\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from tensorflow_addons.metrics import F1Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "posted-minnesota",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SET = 'eanci'\n",
    "\n",
    "'''\n",
    "Ratio defining how much data is reserved for testing. Ex: 0.8 is an 80/20 train/test split\n",
    "Float on interval (0.0, 1.0)\n",
    "'''\n",
    "TRAIN_TEST_SPLIT = 0.9\n",
    "\n",
    "'''\n",
    "Ratio defining how many synthetic minority samples should be created. 1.0 results in a fully balanced set.\n",
    "Float on interval (0.0, 1.0]\n",
    "'''\n",
    "BALANCE_RATIO = 1.0\n",
    "\n",
    "# Number of rounds of active learning to perform\n",
    "AL_ROUNDS = 5\n",
    "\n",
    "# Percent of total valid links to add in each round of active learning\n",
    "# NOTE: Ratio applied to size of full dataset\n",
    "AL_STEP_SIZE = .05 \n",
    "\n",
    "# Number of trials of experiment\n",
    "N_TRIALS = 1 \n",
    "\n",
    "# Training epochs\n",
    "N_EPOCHS=15\n",
    "\n",
    "# Training batch size\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Dimension of the embedding layer. \n",
    "EMBEDDING_DIM = 8\n",
    "\n",
    "# Metrics to meature training performance\n",
    "METRICS = ['loss', 'binary_accuracy', 'recall', 'precision', 'f1_score']\n",
    "\n",
    "# Folder to store TF callback logs\n",
    "# uncomment in loop\n",
    "TENSORBOARD_CALLBACK = tf.keras.callbacks.TensorBoard(log_dir='logs')\n",
    "\n",
    "# Verbosity: 0 = silent, 1 = progress bar, 2 = one line per epoch\n",
    "TRAIN_VERBOSITY = 1\n",
    "TEST_VERBOSITY = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marked-transfer",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATHS = {\n",
    "    'albergate': ('data/albergate/albergate_data.txt', 'data/albergate/albergate_labels.txt'),\n",
    "    'eanci': ('data/eanci/eanci_data.txt', 'data/eanci/eanci_labels.txt'),\n",
    "    'etour': ('data/etour/etour_data.txt', 'data/etour/etour_labels.txt'),\n",
    "    'itrust': ('data/itrust/itrust_data.txt', 'data/itrust/itrust_labels.txt'),\n",
    "    #'kepler': ('data/kepler/kepler_data.txt', 'data/kepler/kepler_labels.txt'),\n",
    "    'modis': ('data/modis/modis_data.txt', 'data/modis/modis_labels.txt'),\n",
    "    'smos': ('data/smos/smos_data.txt', 'data/smos/smos_labels.txt')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compound-result",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Load all datasets and associated labels.\n",
    "'''\n",
    "\n",
    "DATASETS = {}\n",
    "\n",
    "for set_name, paths in DATASET_PATHS.items():\n",
    "    data = []\n",
    "    labels = []\n",
    "    lengths = []\n",
    "    \n",
    "    # Load dataset metadocs\n",
    "    with open(paths[0], newline='') as datafile:\n",
    "        data_reader = csv.reader(datafile, delimiter='\\n')\n",
    "\n",
    "        for row in data_reader:\n",
    "            data.append(row[0])\n",
    "            \n",
    "    # Load dataset labels\n",
    "    with open(paths[1], newline='') as labelfile:\n",
    "        label_reader = csv.reader(labelfile, delimiter='\\n')\n",
    "\n",
    "        for row in label_reader:\n",
    "            labels.append(int(row[0]))\n",
    "            \n",
    "    for seq in data:\n",
    "        lengths.append(len(seq.split()))\n",
    "            \n",
    "    DATASETS[set_name] = (np.array(data, dtype=object), np.array(labels), int(sum(lengths) / len(lengths)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outside-berkeley",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the text vectorization layer to normalize, split, and map strings to integers. \n",
    "vectorize_layer = TextVectorization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naughty-wellington",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEEDS = set()\n",
    "\n",
    "# Generate list of unique random seeds to use with StratifiedShuffleSplit objects\n",
    "while len(RANDOM_SEEDS) < N_TRIALS:\n",
    "    RANDOM_SEEDS.add(np.random.randint(1000))\n",
    "\n",
    "RANDOM_SEEDS = list(RANDOM_SEEDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggregate-interaction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store metric averages for each trial\n",
    "trial_averages = dict([(metric,[]) for metric in METRICS])\n",
    "\n",
    "# Peform N_TRIALS of experiment\n",
    "for i,RANDOM_SEED in enumerate(RANDOM_SEEDS):\n",
    "\n",
    "    # Store metric averages for each fold of a single trial\n",
    "    trial_history = dict([(metric,[]) for metric in ['loss', 'binary_accuracy', 'recall', 'precision', 'f1_score']])\n",
    "    \n",
    "    skf = StratifiedShuffleSplit(n_splits=1, train_size=TRAIN_TEST_SPLIT, random_state=RANDOM_SEED)\n",
    "\n",
    "    print('\\n\\n******************** TRIAL %d ********************' %(i+1))\n",
    "    for train, test in skf.split(DATASETS[TEST_SET][0], DATASETS[TEST_SET][1]):\n",
    "        \n",
    "        total_valid = 0\n",
    "\n",
    "        for label in DATASETS[TEST_SET][1]:\n",
    "            total_valid += label\n",
    "\n",
    "        AL_VALID_LINKS = int(AL_STEP_SIZE * total_valid)\n",
    "        \n",
    "        test_valid = 0\n",
    "        \n",
    "        for index in test:\n",
    "            test_valid += DATASETS[TEST_SET][1][index]\n",
    "            \n",
    "        test_invalid = len(DATASETS[TEST_SET][1]) - valid\n",
    "        \n",
    "        initial = StratifiedShuffleSplit(n_splits=1, train_size=(0.1/0.9), random_state=123)\n",
    "        \n",
    "        for initial_set, remaining_data in initial.split(DATASETS[TEST_SET][0][train], DATASETS[TEST_SET][1][train]):\n",
    "\n",
    "            # Create balanced initial training set\n",
    "            data, labels = utils.balance_data(\n",
    "                DATASETS[TEST_SET][0][initial_set],\n",
    "                DATASETS[TEST_SET][1][initial_set],\n",
    "                DATASETS[TEST_SET][2],\n",
    "                BALANCE_RATIO\n",
    "            )            \n",
    "\n",
    "            for iteration in range(AL_ROUNDS):\n",
    "\n",
    "                # This will cause the model to build an index of strings to integers.\n",
    "                # Per TF: It's important to only use training data when creating vocabulary (using the test set would leak information).\n",
    "                vectorize_layer.set_vocabulary(utils.get_vocabulary(data))\n",
    "\n",
    "                input_dim = len(vectorize_layer.get_vocabulary())\n",
    "\n",
    "                # Embed vocabulary into embedding_dim dimensions.\n",
    "                # Embedding tutorial uses size, Text Classification tutorial uses size + 1 for input_dim\n",
    "                embedding_layer = tf.keras.layers.Embedding(input_dim, EMBEDDING_DIM, name='embedding')\n",
    "\n",
    "                # Define model structure\n",
    "                model = Sequential([\n",
    "                    vectorize_layer,\n",
    "                    embedding_layer,\n",
    "                    #Dropout(0.2),\n",
    "                    GlobalAveragePooling1D(),\n",
    "                    #Dropout(0.2),\n",
    "                    Dense(16, activation='relu'),\n",
    "                    Dense(1, activation='sigmoid')\n",
    "                ])\n",
    "\n",
    "                # Create model\n",
    "                model.compile(\n",
    "                    optimizer='adam',\n",
    "                    loss=tf.keras.losses.BinaryCrossentropy(from_logits=False), # tutorials use true for training, false for production\n",
    "                    metrics=[tf.metrics.BinaryAccuracy(threshold=0.5), tf.keras.metrics.Recall(), tf.keras.metrics.Precision(), F1Score(1, threshold=0.5)]\n",
    "                )\n",
    "\n",
    "                print('\\n\\n*************** TRAIN SET - %d%% ***************' %(iteration*10+10))\n",
    "\n",
    "                print('TRAIN:', len(data))\n",
    "                #if iteration == 0:\n",
    "                #    print('BALANCED TRAIN:', len(data)) \n",
    "                print('TEST:', len(test), test_valid, '(valid)', test_invalid, '(invalid)')\n",
    "                print('REMAIN:', len(remaining_data))\n",
    "\n",
    "                print('\\n******* TRAIN *******')\n",
    "                # Train model\n",
    "                # Verbosity: 0 = silent, 1 = progress bar, 2 = one line per epoch\n",
    "                history = model.fit(\n",
    "                    data,\n",
    "                    labels,\n",
    "                    batch_size=BATCH_SIZE, \n",
    "                    epochs=N_EPOCHS,\n",
    "                    #callbacks=[TENSORBOARD_CALLBACK],\n",
    "                    verbose=TRAIN_VERBOSITY\n",
    "                )\n",
    "\n",
    "                print('\\n******* TEST *******')\n",
    "                # Test \n",
    "                model.evaluate(\n",
    "                        DATASETS[TEST_SET][0][test],\n",
    "                        DATASETS[TEST_SET][1][test],\n",
    "                        verbose=TEST_VERBOSITY\n",
    "                    )\n",
    "\n",
    "                print('\\n******* PREDICT *******')\n",
    "                # Active learning step\n",
    "                predictions = model.predict(DATASETS[TEST_SET][0][remaining_data])\n",
    "\n",
    "                # Find weakest predictions and append them to training set\n",
    "                if iteration < AL_ROUNDS-1:\n",
    "                    predictions = np.concatenate(predictions, axis=0)\n",
    "                    entropies = np.zeros((len(predictions),), dtype=float)\n",
    "\n",
    "                    for i in range(len(predictions)):\n",
    "                        entropies[i] = utils.calculate_entropy(predictions[i])\n",
    "\n",
    "                    weakest = np.argsort(entropies)\n",
    "\n",
    "                    num_added = 0\n",
    "                    to_add = np.zeros((2*AL_VALID_LINKS,), dtype=int)\n",
    "                    \n",
    "                    for i in range(len(weakest) - 1, -1, -1) :\n",
    "                        if DATASETS[TEST_SET][1][remaining_data[weakest[i]]] == 1:\n",
    "                            #print('found pos link')\n",
    "                            to_add[num_added] = weakest[i]\n",
    "                            num_added += 1\n",
    "                        if num_added == AL_VALID_LINKS:\n",
    "                            break\n",
    "\n",
    "                    for i in range(len(weakest) - 1, -1, -1) :\n",
    "                        if DATASETS[TEST_SET][1][remaining_data[weakest[i]]] == 0:\n",
    "                            #print('found neg link')\n",
    "                            to_add[num_added] = weakest[i]\n",
    "                            num_added += 1\n",
    "                        if num_added == 2*AL_VALID_LINKS:\n",
    "                            break\n",
    "\n",
    "                    data = np.concatenate((data, DATASETS[TEST_SET][0][remaining_data[to_add]]), axis=0)\n",
    "                    labels = np.concatenate((labels, DATASETS[TEST_SET][1][remaining_data[to_add]]), axis=0)\n",
    "\n",
    "                    remaining_data = np.setdiff1d(remaining_data, remaining_data[to_add])\n",
    "\n",
    "\n",
    "                #break\n",
    "\n",
    "                #print(history.history.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functional-basketball",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Project_2",
   "language": "python",
   "name": "project_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
