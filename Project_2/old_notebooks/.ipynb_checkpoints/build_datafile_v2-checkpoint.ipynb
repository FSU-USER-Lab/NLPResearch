{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check which language dictionaries are currently available\n",
    "import enchant\n",
    "broker = enchant.Broker()\n",
    "broker.describe()\n",
    "broker.list_languages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "from enchant import Dict\n",
    "from enchant.checker import SpellChecker\n",
    "from nltk import download\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.snowball import EnglishStemmer, ItalianStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download('stopwords')\n",
    "download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_files = os.listdir('data/cc/')\n",
    "uc_files = os.listdir('data/uc/')\n",
    "d = Dict('it')   # create dictionary for Italian\n",
    "ps = PorterStemmer()\n",
    "ss_eng = EnglishStemmer() # Snowball\n",
    "ss_ita = ItalianStemmer() # Snowball"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "- clean contents of all cc files then save in dictionary for easy concatenation\n",
    "- using the cc filenames as dict keys\n",
    "'''\n",
    "\n",
    "cc_data_ps = [] # Porter stemmed data\n",
    "cc_data_ss = [] # Snowball stemmed data\n",
    "cc_dict_ps = {}\n",
    "cc_dict_ss = {}\n",
    "\n",
    "for cc in cc_files:\n",
    "    # convert contents of cc file to a string\n",
    "    cc_data = Path('data/cc/'+cc).read_text(encoding='latin-1')\n",
    "    # tokenize file contents\n",
    "    cc_data = word_tokenize(cc_data)\n",
    "    # remove stop words\n",
    "    cc_data = [word for word in cc_data if not word in stopwords.words()]\n",
    "    # remove tokens which are punctuation or purely numeric\n",
    "    cc_data = [word for word in cc_data if word.isalnum() and not word.isnumeric()]\n",
    "    \n",
    "    # stem tokens based upon token's language \n",
    "    for token in cc_data:\n",
    "        # print('*** %s ***' %cc)\n",
    "        # print(token, '-', d.check(str(token)))\n",
    "        \n",
    "        # True: Italian, False: otherwise\n",
    "        if d.check(token):\n",
    "            cc_data_ss.append(ss_ita.stem(token))\n",
    "        else:\n",
    "            cc_data_ss.append(ss_eng.stem(token))\n",
    "            \n",
    "        # nltk Porter stemmer is language invariant\n",
    "        cc_data_ps.append(ps.stem(token))\n",
    "            \n",
    "    # break\n",
    "    \n",
    "    # convert cleaned data list to string and add to dict using filename as key\n",
    "    cc_dict_ps[cc] = ' '.join(cc_data_ps)\n",
    "    cc_dict_ss[cc] = ' '.join(cc_data_ss)\n",
    "    \n",
    "    cc_data_ps.clear()\n",
    "    cc_data_ss.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "- clean contents of all uc files then save in dictionary for easy concatenation\n",
    "- using the uc filenames as dict keys\n",
    "'''\n",
    "\n",
    "uc_data_ps = [] # Porter stemmed data\n",
    "uc_data_ss = [] # Snowball stemmed data\n",
    "uc_dict_ps = {}\n",
    "uc_dict_ss = {}\n",
    "\n",
    "for uc in uc_files:\n",
    "    # convert contents of uc file to a string\n",
    "    uc_data = Path('data/uc/'+uc).read_text(encoding='latin-1')\n",
    "    # tokenize file contents\n",
    "    uc_data = word_tokenize(uc_data)\n",
    "    # remove stop words\n",
    "    uc_data = [word for word in uc_data if not word in stopwords.words()]\n",
    "    # remove tokens which are punctuation or purely numeric\n",
    "    uc_data = [word for word in uc_data if word.isalnum() and not word.isnumeric()]\n",
    "    \n",
    "    # stem tokens based upon token's language \n",
    "    for token in uc_data:\n",
    "        # print('*** %s ***' %cc)\n",
    "        # print(token, '-', d.check(str(token)))\n",
    "        \n",
    "        # True: Italian, False: otherwise\n",
    "        if d.check(token):\n",
    "            uc_data_ss.append(ss_ita.stem(token))\n",
    "        else:\n",
    "            uc_data_ss.append(ss_eng.stem(token))\n",
    "            \n",
    "        # nltk Porter stemmer is language invariant\n",
    "        uc_data_ps.append(ps.stem(token))\n",
    "            \n",
    "    # break\n",
    "    \n",
    "    # convert cleaned data list to string and add to dict using filename as key\n",
    "    uc_dict_ps[uc] = ' '.join(uc_data_ps)\n",
    "    uc_dict_ss[uc] = ' '.join(uc_data_ss)\n",
    "    \n",
    "    uc_data_ps.clear()\n",
    "    uc_data_ss.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "- iterate through all cc files and for each file iterate through all uc files\n",
    "- find which uc files are attached to each cc by checking smos_oracle\n",
    "'''\n",
    "\n",
    "labeled_list_ps = [] # Porter stemmed data\n",
    "labeled_list_ss = [] # Snowball stemmed data\n",
    "\n",
    "with open('data/smos_oracle.txt', newline='') as oraclefile:\n",
    "    oracle_reader = csv.reader(oraclefile, delimiter=',')\n",
    "        \n",
    "    for row in oracle_reader:\n",
    "        # remove leading whitespace from comma separated values in smos_oracle\n",
    "        for i in range(len(row)):\n",
    "            row[i] = row[i].lstrip()\n",
    "\n",
    "        # cc_dict_ps & cc_dict_ss have identical key sets\n",
    "        for cc_key in cc_dict_ps.keys():\n",
    "            # print('*** CC: %s ***' %cc_key)\n",
    "            \n",
    "            # if the cc filename w/o extension is in the given smos_oracle row\n",
    "            if cc_key.replace('.txt', '') in row:\n",
    "                # uc_dict_ps & uc_dict_ss have identical key sets\n",
    "                for uc_key in uc_dict_ps.keys():\n",
    "                    # if the uc filename w/o extension is in the given smos_oracle row\n",
    "                    if uc_key.replace('.txt', '') in row:\n",
    "                        label = 1\n",
    "                    else:\n",
    "                        label = 0\n",
    "                        \n",
    "                    # print('\\tUC: %s | label: %s' %(uc_key, label))\n",
    "                    \n",
    "                    # save joined data w/ label as a tuple in list for easy stemming    \n",
    "                    labeled_list_ps.append((cc_dict_ps[cc_key] + ' ' + uc_dict_ps[uc_key], label))\n",
    "                    labeled_list_ss.append((cc_dict_ss[cc_key] + ' ' + uc_dict_ss[uc_key], label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Porter stemming output file\n",
    "with open('data/all_links_porter.txt', 'w', newline='') as outfile:\n",
    "    writer = csv.writer(outfile, quoting=csv.QUOTE_MINIMAL)\n",
    "    \n",
    "    for labeled_link in labeled_list_ps:\n",
    "        writer.writerow([labeled_link[0], labeled_link[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Snowball stemming output file\n",
    "with open('data/all_links_snowball.txt', 'w', newline='') as outfile:\n",
    "    writer = csv.writer(outfile, quoting=csv.QUOTE_MINIMAL)\n",
    "    \n",
    "    for labeled_link in labeled_list_ss:\n",
    "        writer.writerow([labeled_link[0], labeled_link[1]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Project_2",
   "language": "python",
   "name": "project_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
