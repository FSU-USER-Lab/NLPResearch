{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import csv\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import utils\n",
    "\n",
    "from tensorflow.keras import Model, Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense, Dropout, Embedding, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from tensorflow_addons.metrics import F1Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = []\n",
    "labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = 'data/smos/smos_data_porter_bal_shuf.txt'\n",
    "label_file = 'data/smos/smos_labels_bal_shuf.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_file, newline='') as datafile:\n",
    "    data_reader = csv.reader(datafile, delimiter='\\n')\n",
    "    \n",
    "    for row in data_reader:\n",
    "        training_data.append(row[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(label_file, newline='') as labelfile:\n",
    "    label_reader = csv.reader(labelfile, delimiter='\\n')\n",
    "    \n",
    "    for row in label_reader:\n",
    "        labels.append(int(row[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and split dataset\n",
    "batch_size = 32\n",
    "seed = 123\n",
    "\n",
    "full_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    'data/smos/train_bal_shuf',\n",
    "    batch_size=batch_size,\n",
    "    label_mode='binary',\n",
    "    seed=seed\n",
    ")\n",
    "\n",
    "train_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    'data/smos/train_bal_shuf',\n",
    "    batch_size=batch_size,\n",
    "    label_mode='binary',\n",
    "    validation_split=0.2, \n",
    "    subset='training',\n",
    "    seed=seed\n",
    ")\n",
    "\n",
    "val_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    'data/smos/train_bal_shuf',\n",
    "    batch_size=batch_size,\n",
    "    label_mode='binary',\n",
    "    validation_split=0.2, \n",
    "    subset='validation',\n",
    "    seed=seed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display 2 examples of points in the dataset\n",
    "for text_batch, label_batch in train_ds.take(1):\n",
    "    for i in range(2):\n",
    "        print(label_batch[i].numpy(), text_batch.numpy()[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the dataset for performance\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lengths = []\n",
    "\n",
    "for seq in training_data:\n",
    "    lengths.append(len(seq.split()))\n",
    "\n",
    "print('Number of metadocuments: ', len(training_data))\n",
    "print('Vocab size: ', utils.vocabulary_size(training_data))\n",
    "print('Avg seq length: ', sum(lengths) / len(lengths))\n",
    "print('Min seq len: ', min(lengths))\n",
    "print('Max seq len: ', max(lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Vocabulary size and number of words in a sequence.\n",
    "If we are using data which has already been balanced then seq len should be set to the max len above\n",
    "since the seq len will have already been set before balancing.\n",
    "'''\n",
    "sequence_length = 200\n",
    "vocab_size = utils.vocabulary_size(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Use the text vectorization layer to normalize, split, and map strings to \n",
    "integers. Note that the layer uses the custom standardization defined above. \n",
    "'''\n",
    "vectorize_layer = TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length)\n",
    "\n",
    "# Make a text-only dataset (no labels) and call adapt to build the vocabulary.\n",
    "text_ds = full_ds.map(lambda x, y: x)\n",
    "vectorize_layer.adapt(text_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimension of the embedding layer. \n",
    "embedding_dim = 8\n",
    "\n",
    "# Embed vocabulary into embedding_dim dimensions.\n",
    "# Embedding tutorial uses size, Text Classification tutorial uses size + 1\n",
    "embedding_layer = tf.keras.layers.Embedding(vocab_size + 1, embedding_dim, name='embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    vectorize_layer,\n",
    "    embedding_layer,\n",
    "    #Dropout(0.2),\n",
    "    GlobalAveragePooling1D(),\n",
    "    #Dropout(0.2),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=[tf.metrics.BinaryAccuracy(threshold=0.5), tf.keras.metrics.Recall(), tf.keras.metrics.Precision()])\n",
    "\n",
    "# tf.metrics.BinaryAccuracy(threshold=0.0)\n",
    "# from_logits=False\n",
    "# tf.keras.metrics.Recall(),\n",
    "# ,F1Score(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir='logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds, \n",
    "    epochs=15,\n",
    "    callbacks=[tensorboard_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    'data/smos/train',\n",
    "    batch_size=batch_size,\n",
    "    label_mode='binary',\n",
    "    seed=seed\n",
    ")\n",
    "\n",
    "original_ds = original_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(original_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the trained word embeddings\n",
    "weights = model.get_layer('embedding').get_weights()[0]\n",
    "vocab = vectorize_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save embeddings to disk\n",
    "out_vec = io.open('data/smos/smos_porter_balanced_vectors.tsv', 'w', encoding='utf-8')\n",
    "out_meta = io.open('data/smos/smos_porter_balanced_metadata.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for index, word in enumerate(vocab):\n",
    "    if  index == 0: continue # skip 0, it's padding.\n",
    "    vec = weights[index] \n",
    "    out_vec.write('\\t'.join([str(x) for x in vec]) + '\\n')\n",
    "    out_meta.write(word + '\\n')\n",
    "    \n",
    "out_vec.close()\n",
    "out_meta.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Project_2",
   "language": "python",
   "name": "project_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
